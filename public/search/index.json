[{"content":"Easy Emacs To start using R, or almost anything else in Emacs you basically need to know 3 things: 1) How to move in Emacs, meaning understanding what is what and learning a few key commands; 2) What is the configuration file and how to use it and 3) How to use packages to extend Emacs. In the first half of this post I will try to show how easy it is to cover these 3 points even for people who are inexperienced in programming. If you don\u0026rsquo;t believe me I invite you to read just the first paragraph of the next section to give you an idea of how easy it really is. During the second half I will show how I\u0026rsquo;m using R in Emacs to give you a starting point of a fully functional environment for R, and will conclude with some topics that can be further explored.\nWhy did I chose Emacs as a researcher in the academia? I started my professional life as a researcher in ecology-related topics. During my master studies I improved my knowledge on statistics considerably and due to that and to the complexity of my research project, I did not want to use a GUI-based software for my statistical analysis. Thus, I started learning R, and believe it or not, I completed my research project for my Thesis by tipping R code directly to the console from my handwritten notes. When I started my PhD I thought that it would be easier to just write the code I need in electronic format and copy-paste it to the R console. And with that idea in mind and the help of the internet, I discovered the text editors and Emacs, and a whole new universe opened up to me. I know that many in my position would be ashamed of sharing such a story but I simply want to exemplify how easy it is to start using Emacs, contrary to the popular belief. I went from having no idea of what a text editor is, to setting up and using Emacs with R, with no intermediate steps.\nEmacs is a wonderful text editor that can easily be extended to do many things. You can have tools to help in writing your code such as different types of indentation, syntax highlighter, git utilities, project management, code maps, web browser, even to play games. Emacs provides by default a lot of functionalities to move easily through the text files, including keybindings to go to the end and beginning of buffer, function or paragraph, parentheses matching, text search, exploration and replacement, syntax and spelling checks. You can create markers to move quickly to particular files, window configurations or to store text and numbers. Some consider Emacs almost as an OS because you can also do things like create and delete files, version control, internet browser, and more.\nThe reason why I stayed with Emacs as a researcher in the academia was mainly due to org-mode. It is an Emacs major mode that helped me to organize my research and still today it helps me to organize my job. You can think if it as the Emacs version of Markdown, with the possibility to organize to-do lists, tag notes and sections, fully organize an agenda (tracking tasks, set deadlines, schedule items, etc.). You can add chunks of code from almost any language and, with the help of a couple more libraries, you can run the code within the org file itself. Github and other git servers have integrated tools to view org files as html, but there are libraries to convert them also to pdf, libreoffice, create presentations and more.\nAnother important point that made me fall in love with Emacs was the fact that, if I managed to keep most of my research files as text I could do it all from Emacs, instead of using different apps for different tasks. And so I did: I was writing my papers in LaTeX and organizing my bibliography with bibtex; I was saving data as CSV which Emacs can manage very well; the graphics were more of an issue but, since I used R to create most of them, I simply needed to save the right script for the right plot. And all this was organized in org-mode with links to this or that file according to the project, section, tag, etc. And the reason why I wanted to do this was not even for organization purposes, but rather because, as text, I could track all my changes using Git, which ended up being a huge support for my PhD work: I could revert changes if I had mistakes or explore old commits, and backup all of that easily. So, at the end, while R had been the reason why I decided to explore Emacs, it was in fact the combo Emacs + org-mode + git which improved my organization and productivity potentially during my research life. And I would like to share this tools with as many people as possible.\nThus, I decided to create this post, to give you an idea of how easily you can start using Emacs for R coding. If you enjoy it and you\u0026rsquo;d like me to create more content about some of the tools briefly described here, make sure to leave me a comment and I\u0026rsquo;ll take care of it. I include a general list of the tools I use regularly in Emacs at the end, you can have a look there.\nQuick start Although Emacs is extremely customizable, it is true that it requires some coding skills and knowledge of the not so popular programming language called Emacs Lisp. You would probably have also read that Emacs has a very steep learning curve, which is also true. This two conditions usually scare people away from learning Emacs. In this section I will demonstrate that you don\u0026rsquo;t need to know Emacs Lisp (or programming at all) and that with very little knowledge of Emacs you can have a ready-to-use super-powerful R editor.\nThis chapter is a brief overview of the rest of the post meant as a quick start to get Emacs up and working with R in just as few as 10 steps. The rest of the post will simply go deeper into each of the steps.\nMake sure that you have installed both, Emacs and R in your computer.\nOpen Emacs and press the keys Ctr + x, release and press Ctr + f (in Emacs notation, this combination of keys is expressed as \u0026ldquo;C-x C-f\u0026rdquo;). Focus on the mini buffer, it is the line positioned at the bottom of your window. It is waiting for you to type something. If there is some path to a folder already in that area delete it first and then type ~/.emacs and enter. It should open a new empty window.\nThis is your configuration file. Paste the following code in your new window (require 'package) (add-to-list 'package-archives '(\u0026quot;melpa\u0026quot; . \u0026quot;https://melpa.org/packages/\u0026quot;)) (unless (package-installed-p 'use-package) (package-refresh-contents) (package-install 'use-package)) (setq use-package-always-ensure t) (use-package ess) (use-package company :config (add-hook 'after-init-hook 'global-company-mode)) (setq company-selection-wrap-around t company-tooltip-align-annotations t company-idle-delay 0.45 company-minimum-prefix-length 3 company-tooltip-limit 10) This configuration assumes that you have installed R with all the defaults. If you have installed R in a directory of your choice, add the following line at the end of the configuration file, changing the path of my example for the path were you have installed R.\n(setq inferior-ess-r-program \u0026quot;C:/Users/Manuel/path_where_R_is/R-4.2.1/bin/R.exe\u0026quot;) Type C-x C-s (meaning, Ctr + x, release, Ctr + s). This will save the file.\nType now Alt + x (in Mac command key instead of Alt or, if it does not work, the option key instead), this is the key Meta, represented in Emacs by M-x. At this point you want to focus again on your mini buffer, the line at the bottom of the screen.\nType there package-install enter and then type use-package, enter. If some text appears at the bottom of your .emacs file don\u0026rsquo;t worry, it is intended this way.\nFocus on the mini buffer in case it prompts something. If it asks you if you want to install the package type y and enter. If it tells you that it cannot find the package or it does not exist, close Emacs, open it again and repeat steps 5 and 6. It should show a message informing that it has been installed. In my case it shows the following line in the minibuffer:\nDone (Total of 9 files compiled, 4 skipped) Now close Emacs and open it again. This time it should take longer to load. Be patient, Emacs is loading installing and loading more packages for you.\nType again C-x C-f and type test.R, enter. You can change the path before the file if you wish (i.e. ~/Code/test.R).\nA new empty area should appear. Type there one line of R code. When you are done, while keeping the cursor in the line where your code is, press C-c C-j, this sends a line to R. A new area will open, showing the R console and the results of the code you just sent. If nothing happens focus on the minibuffer, it might ask you where to start your R session; you can just press enter or provide a new location. Then you can continue typing R code and use the same combination of keys to run a line, you can use C-c C-p to run a paragraph, C-c C-f to send a function and C-c C-b the send the whole buffer (which here basically means the whole file) or simply Control + enter (C-return) to send any of the mentioned regions. And as you already know, you can save the file by pressing C-x C-s.\nIf everything went well now you should have a simple Emacs configuration to start coding in R. Congratulations!.\nGetting started with Emacs Installation and first steps Both, R and Emacs are extremely easy to install, therefore I will not go into the details for it. Basically in any Linux distribution you can just use your package manager for it, in windows just download and run the official executable files and for Mac you can also download the binaries or use alternative package methods like homebrew (also applicable for Linux). For R go to (https://www.r-project.org/) and for Emacs to (https://www.gnu.org/software/emacs/).\nOnce you have installed Emacs you can run it and you will have the welcome screen, together with some toolbars and list of menus. At this point you could basically use Emacs like any other text editor: you can open files, edit them and save them by using all the menus, icons and your mouse. However, the real power of Emacs rest in its keybindings. To get started I recommend to click on the link Emacs Tutorial of the welcome screen, it will guide you through the basics. After the tutorial you will feel more comfortable finding your way around Emacs and the rest of this post will be easier to follow.\nControl, Meta and the minibuffer, moving in Emacs When you are working with text most of the time (as it is the case of R code) the use of the mouse can reduce productivity by searching with your eyes the exact places you want to mark, going all the way to the menu to save or open a file, finding when a parentheses opens and closes, and so on. The idea of the keybindings is to increase productivity by staying in the text at the level of the keyboard most of the time, since this is what we actually do when we write code.\nAt the beginning it can be complicated to memorize so many keybindings. I\u0026rsquo;d recommend to try to remember the most basic ones to move along the text, save files, close Emacs and split the screen as you need to. The rest can be easily achieved through the mouse icons and menus. When I started using Emacs I was having a piece of paper with the most useful keybindings and, as my fingers started remembering by themselves I was deleting those and adding new ones. Today I can assure you that my productivity to write R code is much better than it ever was with any other text editor.\nI will not go through the details of which keybindings do what since it is all in the self contained tutorial, however there are some key points to learn the keybindings. One is the knowledge of the so called \u0026ldquo;Emacs Notation\u0026rdquo;. Whenever you search either in Emacs documentation or some other sources to use Emacs, how to perform certain actions, you will find things like C-M-a. The capital C is short for the key Control, while capital M is for meta, which in most computers is Alt and in Mac is usually the key Command. Thus, C-M-a would mean that you have to hold the key Control, the key Meta and the key a. Usually the keys Control and Meta are used in combination with other keys and thus, the letters C and M are used at the beginning of the commands. That would mean that, for example, the combination C-C does not mean Control twice, but rather Control plus capital C. Although this rarely happens (I\u0026rsquo;ve never used such a combination), it is important to be aware because Emacs recognizes difference between upper and lower case.\nAnother important part to know in Emacs is the minibuffer. By default it is positioned at the bottom of the screen and it is used to communicate commands between Emacs and the user. For example, when you save a file the minibuffer will print something like Wrote /path/to/file.R to signal that the file has been saved.\nThe minibuffer is also used to pass commands to Emacs. All the keybindings are bind to a command, although not every command is bind to a key. To pass a command to Emacs you can use the keys M-x. As an example you can try to use M-x and you will see that the minibuffer has changed and now is ready to receive your input. Type there help-for-help and a new menu will appear, showing you the options for help and the instructions to use it. For example, type b to display all keybindings. The command help-for-help is bind to the keys C-h ? therefore, if you would type this combination instead you would have the same response.\nEmacs uses intuitive key bindings and thus, the combination C-h is designed for help. For example, the combination C-h b will show the help for the Bindings, C-h t help with Tutorial, C-h f help for a Function (you have to type in the function), etc. C-x executes high levels functions such as save file C-x C-s or close Emacs with C-x C-c.\nYou can take some time to familiarize yourself with some of the keybindings and later you will see how it pays off when writing and executing R code. The best way to get familiar with the main ones is by following the tutorial included in Emacs, you have the Link in the welcome page, in the Help menu or simply type C-h t.\nThe configuration file The second part of the power of Emacs is its customization. The first aspect for its customization is the init file, also known as dot Emacs. According to its documentation:\nWhen Emacs is started, it normally tries to load a Lisp program from an initialization file, or init file for short. This file, if it exists, specifies how to initialize Emacs for you. Traditionally, file ~/.emacs is used as the init file, although Emacs also looks at ~/.emacs.el, ~/.emacs.d/init.el, ~/.config/emacs/init.el, or other locations. See How Emacs Finds Your Init File.\nThis means that you have several options to tell Emacs how to start. If you are not familiar with Unix style, ~/ is the home directory. That means that you can have your configuration file in your home directory called .emacs or .emacs.el, or you can have it inside a configuration folder ~/.emacs.d/ or ~/.config/emacs/ with the name init.el (or some other options, see the link in the quote).\nTo keep consistency and facility, we will keep the same approach that we used in the quick guide above and use the file dot emacs.\nOpen Emacs and press the keys Ctr + x, release and press Ctr + f (in Emacs notation, this combination of keys is expressed as \u0026ldquo;C-x C-f\u0026rdquo;). Focus on the mini buffer, it is the line positioned at the bottom of your window. It is waiting for you to type something. If there is some path to a folder already in that area delete it first and then type ~/.emacs and enter. It should open a new empty window. If you followed step 2 from within Emacs you should have now an empty screen where you can start typing Elisp code to tell Emacs how to start. After a new installation the file still does not exists (although you might already have created it if you followed the quick start). With the command C-x C-f we can create it. Make sure that it is stored in the home folder ~/ and not somewhere else. To demonstrate the point, type the following line anywhere in your .emacs file: (setq inhibit-startup-screen t), that tells Emacs to inhibit the startup screen. Now save it with C-x C-s, close Emacs and open it again and now the startup screen showing you the tutorial should not be there anymore. If you still want to see the welcome screen at startup you can simply delete that line and the startup screen will be back (C-x C-f type ~/.emacs, delete the line and save).\nHere are just a couple of lines that are useful to add to your dot Emacs file for writing R code:\n;; enable column numbers (setq column-number-mode t) (add-hook 'prog-mode-hook 'display-line-numbers-mode) ;; Highlights the matching parentheses (show-paren-mode 1) ;; Using arrow for moving through buffers (global-set-key (kbd \u0026quot;C-x \u0026lt;up\u0026gt;\u0026quot;) 'windmove-up) (global-set-key (kbd \u0026quot;C-x \u0026lt;down\u0026gt;\u0026quot;) 'windmove-down) (global-set-key (kbd \u0026quot;C-x \u0026lt;left\u0026gt;\u0026quot;) 'windmove-left) (global-set-key (kbd \u0026quot;C-x \u0026lt;right\u0026gt;\u0026quot;) 'windmove-right) ;; Starting file (setq initial-buffer-choice (lambda () (if (buffer-file-name) (current-buffer) (find-file \u0026quot;~/Path/to_your_file/a_starting_file.R\u0026quot;)))) The first part will simply enable column numbers when writing code, for some reason Emacs does not do it by default. Next we are activating the show-paren-mode which highlights the matching parentheses, a useful function when writing long functions. The third paragraph will allow you to move between buffers using C-x and the arrows. For example, you can split buffer horizontally using C-x 2 and then move to the lower buffer using C-x and down arrow, or back to the upper with the upper arrow C-x . The last part can set a custom starting file, meaning each time you open Emacs this will be the file that will open by default, but if you open a different file using Emacs this starting file won\u0026rsquo;t show up.\nExtending Emacs: packages Emacs can be fully customized in the sense that you can write Elisp code to get Emacs do what you want. Luckily, you don\u0026rsquo;t need to know Elisp to take advantage of it. Like in R, there are several packages that extend the basic Emacs to do more than it was originally designed to. In the quick start section steps 3 to 7 we did exactly that in 2 different ways. Let\u0026rsquo;s take a look at each option and detail to install packages.\nELPA and MELPA ELPA is the Emacs Lisp Package Archive, written originally by TomTromey. It is included in GnuEmacs, starting with version 24. package.el is the package manager library for ELPA.\n“Our goal is to make it simple to install, use, and upgrade Emacs Lisp packages. We supply package.el a simple package manager for Emacs, and a repository of pre-packed Emacs Lisp code.”\nSee InstallingPackages for basic usage information.\nTo see the ELPA packages available you can execute the command list-packages (remember, by using M-x). However, sometimes this are not the most up to date versions, or some packages are simply not listed in the ELPA repositories but rather in MELPA only.\nMELPA is an ELPA-compatible package repository that contains an enormous number of useful Emacs packages.\nIn contrast to ELPA, Emacs is not configured by default to install packages from MELPA. You will have to configure Emacs to use it.\nYou can think of MELPA to ELPA like Bioconductor is to CRAN. In their own words, this is what MELPA is intended for:\nUp-to-date packages built on our servers from upstream source\nInstallable in any Emacs with \u0026lsquo;package.el\u0026rsquo; - no local version-control tools needed\nCurated - no obsolete, renamed, forked or randomly hacked packages\nComprehensive - more packages than any other archive\nAutomatic updates - new commits result in new packages\nExtensible - contribute new recipes, and we\u0026rsquo;ll build the packages\nTo configure Emacs to find MELPA packages we simply need two lines of code in our configuration file.\n(require 'package) (add-to-list 'package-archives '(\u0026quot;melpa\u0026quot; . \u0026quot;https://melpa.org/packages/\u0026quot;)) Add those lines to your dot emacs file, save it and restart Emacs to take effect. Now, upon calling list-packages you should see an extended list of packages, some of which are tagged as \u0026ldquo;melpa\u0026rdquo; in the section \u0026ldquo;Archive\u0026rdquo; of the list.\nlist-packages and install-package From the last section you already know how to call list-packages and if you followed the quick start, you also know how to use the command install-package. Basically, to install a package you could call the command install-package, RET and type the exact name of the package, which can be found in the list of the packages.\nBut there is more. According to the Emacs Documentation:\nThe command M-x list-packages brings up the package menu. This is a buffer listing all the packages that Emacs knows about, one on each line, with the following information:\nThe package name (e.g., ‘auctex’). The package’s version number (e.g., ‘11.86’). The package’s status—normally one of ‘available’ (can be downloaded from the package archive), ‘installed’, or ‘built-in’ (included in Emacs by default). See Package Statuses. Which package archive this package is from, if you have more than one package archive enabled. A short description of the package.\nEach area in Emacs is called a buffer and depending what the buffer is running it will be controlled by its own rules. As you saw in the quick start, we can send a line of R code to the terminal by typing C-c C-j, but such key combination won\u0026rsquo;t work the same if we are not inside an R file. In the same way, the buffer listing the packages has its own keybindings. You can find all the details in the link above, but here are the most useful ones:\nMove along the buffer using the arrow keys. Move one page down using C-v and one page up with M-v. Search for text using C-s. Press i to mark a package for installation. Press u to unmark a package. Press x to execute marked actions. Or simply use the menu \u0026ldquo;Package\u0026rdquo; To exit you can type q or you can kill this or any buffer by typing C-x k and then RET.\nuse-package Another way to install packages is by using the package use-package which in short is a package manager.\nThe use-package macro allows you to isolate package configuration in your .emacs file in a way that is both performance-oriented and, well, tidy. I created it because I have over 80 packages that I use in Emacs, and things were getting difficult to manage. Yet with this utility my total load time is around 2 seconds, with no loss of functionality!\nLet\u0026rsquo;s use the example from the quick start, step 3:\n(unless (package-installed-p 'use-package) (package-refresh-contents) (package-install 'use-package)) (setq use-package-always-ensure t) (use-package ess) The first part makes sure that the package use-package is installed and to refresh the list of packages based on use-package own rules. The second part ensures that the package will be installed if it was not yet installed. In other words, it makes the installation of the packages automatic so, you don\u0026rsquo;t have to use install-package command of the list-packages menu. Finally (use-package ess) loads the package ess to Emacs, which is the package responsible for running R. Final remarks about Emacs configuration The detail usage of use-package is quite complex, especially for a new Emacs user and it is not covered in this post. Likewise, a more detailed configuration of the init file (.emacs) and the customization of Emacs and the packages through it can take an entire manual. If you are really interested you can start by following the links provided so far. Otherwise I would recommend staying with the basis presented here, getting familiar with Emacs and slowly getting deeper into particular topics. The info presented here is just the very basics to get started with a simple yet powerful IDE for R.\nOne important point to know though is that usually, after installing a package, it has to be loaded through the init file so that Emacs can use it. Usually you can find detailed info in the documentation and/or website of the particular package on how to load it and how to configure it. The general rule is to load it using the base Emacs function require (i.e., (require 'ess)) or alternatively with use-package (i.e., (use-package ess)).\nESS to speak with R As it was already mentioned, ESS is the Emacs package used for R code. It stands for \u0026ldquo;Emacs Speaks Statistics\u0026rdquo; and it can run not only R code but other statistical analysis programs including Julia.\nEmacs Speaks Statistics (ESS) is an add-on package for GNU Emacs. It is designed to support editing of scripts and interaction with various statistical analysis programs such as R, S-Plus, SAS, Stata and OpenBUGS/JAGS. Although all users of these statistical analysis programs are welcome to apply ESS, advanced users or professionals who regularly work with text-based statistical analysis scripts, with various statistical languages/programs, or with different operating systems might benefit from it the most.\nThe rationale for developing ESS is that most statistical analysis systems provide a more or less sophisticated graphical user interface (GUI). However, their full power is only available using their scripting language. Furthermore, complex statistical analysis projects require a high degree of automation and documentation which can only be handled by creating statistical analysis scripts. Unfortunately, many statistics packages provide only weak text editor functionality and show major differences between them. Without a unified text editor user interface additional effort is required from the user to cope with limited functionality and with text editor differences.\nESS is a very powerful and specialized software on its own, its documentation includes 16 detailed topics for its usage. Its use with Emacs can be compared to R Studio alone, although there are significant differences, the ESS team have also work a lot lately on having enough similarities to make R Studio users feel comfortable switching to Emacs.\nI use it particularly for R, it helps me to write R code including syntax highlight and indentation, to send R code to the console, to debug R code and more.\nHow to use R in ESS As we already mentioned, Emacs can be fully configured to our needs and wishes. If you clicked in the links above, you can also see that ESS documentation is quite long and complex. The present post is merely an introduction to its possibilities. Here is a table with the most commonly used key bindings and commands used in ESS.\nKeys Effect C-RET Sends region, line or step to the console C-c C-j Sends line to the console C-c C-p Sends paragraph to the console C-c C-b Sends buffer (whole file) to the console C-c C-f Sends buffer to the console M-x ess-indent-exp Indents expression To use them make sure to have installed and loaded ESS in your Emacs. Then you can simply create an R file, start typing code and run it.\nYou can also use the menu \u0026ldquo;ESS\u0026rdquo; fro within the R buffer to explore more keybindings and commands. One useful section is the \u0026ldquo;Font Lock\u0026rdquo; which defines the Syntax Highlighting for R. I\u0026rsquo;d recommend to have open a relatively long or complex R script and mark/unmark fields to see what happens. But basically, the fields marked in the menu \u0026ldquo;Font Lock\u0026rdquo; are the fields that will be highlighted by Emacs.\nThe ESS debugging tool is also useful and powerful. You can simply type in you R console debug(function) and then run the function called inside debug or a function containing it and Emacs will run step by step and side by side the file each time you type RET in the console. Whenever you don\u0026rsquo;t type RET you can do all sort of stuff locally such as print the state of an argument or even change its value.\nCompany Among all the libraries and Emacs functionalities that can help us writing R code, I think that Company deserves a special mention. It is an auto completion tool that is easy to set up for ESS and intuitive to use. If you followed the quick start you should already have it installed and ready to use.\n(use-package company :config (add-hook 'after-init-hook 'global-company-mode)) (setq company-selection-wrap-around t company-tooltip-align-annotations t company-idle-delay 0.45 company-minimum-prefix-length 3 company-tooltip-limit 10) The first paragraph is calling the library and creating a hook to activate it globally. You could as well change the hook to have it active only when ESS is running, but in my experience it is quite useful to have it active globally.\nThe second paragraph customize some of its functionality, for example company-idle-delay defines the delay time to show the autocomplete menu, in seconds. You can fin more info about it in the official documentation or simply by typing C-h v RET and the name of the variable (i.e., C-h v RET company-idle-delay).\nIf you followed the quick start you could probably had already noticed that you get code suggestions while typing R code. If not, I recommend you to give it a try. The variable company-minimum-prefix-length is set to 3, which means that you need to type at least 3 characters and wait 0.45 seconds for the menu to pop-up.\nWhat next? - Explore Emacs and its libraries As mentioned before, Emacs has many functionalities that can help boosting your productivity and writing code more easily. Here are some I personally use:\nEmacs Functionalities Purpose org-mode Organization functionality in Emacs using plain text paren-mode Commands for editing with parentheses vc-mode Version Conrol in Emacs csv-mode Visualize and edit CSV files bookmarks and registers Save position in a file, windows configuration or text in keystrokes If you had a look at list-packages you would have noticed that the number of libraries available is huge. Here is a very conservative list of libraries that are particularly useful for working with R, or code in general.\nPackage Use polymode Helps for markdown documents poly-R Polymode for R poly-markdown Polymode for markdown Magit A more user friendly Version control with great visualizations Flyspell Syntax check. Uses lintr for R Swiper The link includes Ivy for auto completion, Counsel for common Emacs commands and Swiper for search Yasnippet Templates system for Emacs Emacs is also an excellent tool for different kinds of professional writting, during my PhD studies I was using AUCTeX for writing papers in LaTeX, supported by bibtex-mode to organize the bibliography and helm-bibtex for queries. Emacs can also run web browsers, games and functionalities for email, among others. I personally don\u0026rsquo;t use these much, but it shows the great possibilities of Emacs.\nIf you would like me to cover some of them in more detail leave a comment and I\u0026rsquo;ll try my best to share my knowledge to help.\n","date":"2022-12-29","permalink":"https://blog.rwhitedwarf.com/post/use_emacs_for_r/","tags":["R basics","R tips","emacs"],"title":"Using Emacs for R"},{"content":"Scope of this post This is the second part of the series to create a map of any region of the world with R.\nWe are creating maps of data showing changes over a span of time for different countries and pointing at all kinds of cities. That basically means that we need to map any region of the world with R. Today there are all kinds of packages and techniques to do that. I will share the strategy I used with ggplot2 and maps packages, using support of Open Street Map to obtain the coordinates of cities and finally making it interactive with shiny. The project is quite long for a single post, so my idea is to split it into a few smaller blog posts. So far, the list is a follows:\nThe basic map Web scrapping with nominatim open street maps Maps with cities Dynamic maps in time Making a single script for fast replication Making the code interactive in a shiny app The ideas is to share the how-to of one of the projects I am most proud of and, at the same time to give back to the R community in hopes that it can help somebody else.\nI hope you all enjoy it. Feel free to leave any kind of comment and/or question at the end.\nOpen Street Maps and Nominatim A simple query\nlibrary('RJSONIO') site \u0026lt;- (\u0026quot;http://nominatim.openstreetmap.org/search?city=Texcoco\u0026amp;limit=9\u0026amp;format=json\u0026quot;) (result \u0026lt;- fromJSON(site)) \u0026gt; [[1]] \u0026gt; [[1]]$place_id \u0026gt; [1] 1177116 \u0026gt; \u0026gt; [[1]]$licence \u0026gt; [1] \u0026quot;Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright\u0026quot; \u0026gt; \u0026gt; [[1]]$osm_type \u0026gt; [1] \u0026quot;node\u0026quot; \u0026gt; \u0026gt; [[1]]$osm_id \u0026gt; [1] 336169214 \u0026gt; \u0026gt; [[1]]$boundingbox \u0026gt; [1] \u0026quot;29.619\u0026quot; \u0026quot;29.659\u0026quot; \u0026quot;-111.0786667\u0026quot; \u0026quot;-111.0386667\u0026quot; \u0026gt; \u0026gt; [[1]]$lat \u0026gt; [1] \u0026quot;29.639\u0026quot; \u0026gt; \u0026gt; [[1]]$lon \u0026gt; [1] \u0026quot;-111.0586667\u0026quot; \u0026gt; \u0026gt; [[1]]$display_name \u0026gt; [1] \u0026quot;Texcoco, Carbó, Sonora, México\u0026quot; \u0026gt; \u0026gt; [[1]]$class \u0026gt; [1] \u0026quot;place\u0026quot; \u0026gt; \u0026gt; [[1]]$type \u0026gt; [1] \u0026quot;village\u0026quot; \u0026gt; \u0026gt; [[1]]$importance \u0026gt; [1] 0.385 \u0026gt; \u0026gt; [[1]]$icon \u0026gt; [1] \u0026quot;https://nominatim.openstreetmap.org/ui/mapicons/poi_place_village.p.20.png\u0026quot; \u0026gt; \u0026gt; \u0026gt; [[2]] \u0026gt; [[2]]$place_id \u0026gt; [1] 3448536 \u0026gt; \u0026gt; [[2]]$licence \u0026gt; [1] \u0026quot;Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright\u0026quot; \u0026gt; \u0026gt; [[2]]$osm_type \u0026gt; [1] \u0026quot;node\u0026quot; \u0026gt; \u0026gt; [[2]]$osm_id \u0026gt; [1] 458633446 \u0026gt; \u0026gt; [[2]]$boundingbox \u0026gt; [1] \u0026quot;16.551667\u0026quot; \u0026quot;16.591667\u0026quot; \u0026quot;-97.053333\u0026quot; \u0026quot;-97.013333\u0026quot; \u0026gt; \u0026gt; [[2]]$lat \u0026gt; [1] \u0026quot;16.571667\u0026quot; \u0026gt; \u0026gt; [[2]]$lon \u0026gt; [1] \u0026quot;-97.033333\u0026quot; \u0026gt; \u0026gt; [[2]]$display_name \u0026gt; [1] \u0026quot;Texcoco, Santa María Sola, Oaxaca, México\u0026quot; \u0026gt; \u0026gt; [[2]]$class \u0026gt; [1] \u0026quot;place\u0026quot; \u0026gt; \u0026gt; [[2]]$type \u0026gt; [1] \u0026quot;hamlet\u0026quot; \u0026gt; \u0026gt; [[2]]$importance \u0026gt; [1] 0.36 \u0026gt; \u0026gt; [[2]]$icon \u0026gt; [1] \u0026quot;https://nominatim.openstreetmap.org/ui/mapicons/poi_place_village.p.20.png\u0026quot; We start with Open Street Map and its API nominatim. In the piece of code above we can see how to perform a simple query for one city. It is basically one long string containing first the url of nominatim and at the end the search details: here we start the search with city using ?city=Texcoco, in this case I aimed for a city with only a few results. Next we are limiting the amount of results to 9 with \u0026amp;limit=9 and finally requesting the results in format JSON.\nWe could basically copy the string that we are passing to site and paste it in the web browser to see the results directly there. Feel free to change the city Texcoco to any other city, and play a bit more with the rest of the parameters. Particularly have a look at what happens when you remove the \u0026amp;format=json part or when you exchange json for any other abstract string like csv or other non-recognized format.\nA more specific query\ncity \u0026lt;- 'San%20Francisco' state \u0026lt;- '\u0026amp;state=California' country \u0026lt;- '\u0026amp;countrycodes=US' start.nominatim \u0026lt;- \u0026quot;http://nominatim.openstreetmap.org/search?city=\u0026quot; end.nominatim \u0026lt;- \u0026quot;\u0026amp;format=json\u0026quot; site \u0026lt;- paste0(start.nominatim, city, country, state, end.nominatim) (result \u0026lt;- fromJSON(site)) \u0026gt; [[1]] \u0026gt; [[1]]$place_id \u0026gt; [1] 297054975 \u0026gt; \u0026gt; [[1]]$licence \u0026gt; [1] \u0026quot;Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright\u0026quot; \u0026gt; \u0026gt; [[1]]$osm_type \u0026gt; [1] \u0026quot;relation\u0026quot; \u0026gt; \u0026gt; [[1]]$osm_id \u0026gt; [1] 111968 \u0026gt; \u0026gt; [[1]]$boundingbox \u0026gt; [1] \u0026quot;37.6403143\u0026quot; \u0026quot;37.929811\u0026quot; \u0026quot;-123.173825\u0026quot; \u0026quot;-122.281479\u0026quot; \u0026gt; \u0026gt; [[1]]$lat \u0026gt; [1] \u0026quot;37.7790262\u0026quot; \u0026gt; \u0026gt; [[1]]$lon \u0026gt; [1] \u0026quot;-122.419906\u0026quot; \u0026gt; \u0026gt; [[1]]$display_name \u0026gt; [1] \u0026quot;San Francisco, CAL Fire Northern Region, California, United States\u0026quot; \u0026gt; \u0026gt; [[1]]$class \u0026gt; [1] \u0026quot;boundary\u0026quot; \u0026gt; \u0026gt; [[1]]$type \u0026gt; [1] \u0026quot;administrative\u0026quot; \u0026gt; \u0026gt; [[1]]$importance \u0026gt; [1] 1.035131 \u0026gt; \u0026gt; [[1]]$icon \u0026gt; [1] \u0026quot;https://nominatim.openstreetmap.org/ui/mapicons/poi_boundary_administrative.p.20.png\u0026quot; If you explore OSM and nominatim a bit you will see that we can add search arguments using \u0026amp; followed by the argument we want (i.e., state), the symbol equal = and the argument. In my example above you can see how we are specifying the State and Country of our query. Additionally it is important to know how to pass spaces in a name, for example, San Francisco will be passed as San%20Francisco.\nWith this basic information in mind and knowing that the package RJSONIO helps us to retrieve the data from the JSON api into an R-friendly format, we can easily prepare a function to search for any city quickly, provided a few extra details like a region, state or county, and especially important, the country (try searching for cities like London or Prague without providing a country, you might be surprised of how many cities exist in the world with such names).\ncoords_from_city \u0026lt;- function(City, CountryTwoLetter, Region = NULL, State = NULL, County = NULL){ require('RJSONIO') CityCoded \u0026lt;- gsub(' ','%20',City) #remove space for URLs CountryCoded \u0026lt;- paste(\u0026quot;\u0026amp;countrycodes=\u0026quot;, CountryTwoLetter, sep = '') extras \u0026lt;- c(state = State, region = Region, county = County) extrasCoded \u0026lt;- '' if(!is.null(extras)) { for(i in 1:length(extras)){ if(extras[i] != '' \u0026amp;\u0026amp; !is.na(extras[i]) \u0026amp;\u0026amp; !grepl(\u0026quot;^\\\\s*$\u0026quot;, extras[i])){ valCoded \u0026lt;- gsub(' ', '%20', extras[i]) extrasCoded \u0026lt;- paste0(extrasCoded, '\u0026amp;', names(extras)[i], '=', valCoded) } } } ## get data url \u0026lt;- paste( \u0026quot;http://nominatim.openstreetmap.org/search?city=\u0026quot; , CityCoded , CountryCoded , extrasCoded , \u0026quot;\u0026amp;format=json\u0026quot; , sep=\u0026quot;\u0026quot;) x \u0026lt;- fromJSON(url) ## retrieve coords if(is.vector(x)){ message(paste('Found', x[[1]]$display_name)) lon \u0026lt;- x[[1]]$lon lat \u0026lt;- x[[1]]$lat osm_name \u0026lt;- x[[1]]$display_name coords \u0026lt;- data.frame('lon' = lon, 'lat' = lat, 'osm_name' = osm_name) } ## When x is not a vector else{ message(paste('No results found for', City, CountryTwoLetter)) coords \u0026lt;- data.frame('lon' = NA, 'lat' = NA) } ## return a df coords } An important detail to know is that often, providing values to either state or region parameters returns similar results, this is particularly useful in countries where no states are used or other forms of organization are present. However, when the country has \u0026ldquo;States\u0026rdquo;, you cannot pass the name of a State to the parameter Region.\nThe function returns a data frame that we will use later to create a table with all of our results. Since we are interested in creating maps, we only need the coordinates expressed in latitude and longitude parameters. In case the query is not found, it fills the values with NA\u0026rsquo;s, which later we\u0026rsquo;ll use to keep track of what was found and what wasn\u0026rsquo;t. We are also keeping the values inside osm_name which provides enough information to tell the user useful details regarding the search results, including the country of the city found, and other details like state or region.\nAn important point to consider in coords_from_city is that it will return only the top result from the query. It means that the more information you provide, the more accurate your result will be. For our project it worked well because for big countries we were always collecting enough info about regions and states, while for smaller countries often the options were too small. But if you use the function it is important to know that if you provide a city name like Springfield, Country = 'US' and give no info about State and County, the function will retrieve only the top result from the search and discard the remaining options.\nKeeping the info in a database The function coords_from_city could be enough if we need to retrieve info about a few cities; we could make a for loop, retrieve all the coords we need and sore them in a data frame to later save as csv, Rdata or any format we choose. The same is true when we are searching for hundreds or thousands of cities but with data increasing the searching time also increases. If, for any reason, the R session breaks, the information would be lost and we will have to start all over again from row 1. Therefore, we are going to send every single result to a database. In that way, no matter when we stop the process or how this happens, the data is safely stored outside of R.\nwebscrap_to_sqlite \u0026lt;- function(db.name, dat, col.city = 'City', col.country = 'Country', region = NULL, state = NULL, county = NULL) { require(RSQLite) df_len \u0026lt;- nrow(dat) ## Connect to db and table con \u0026lt;- dbConnect(drv=SQLite(), dbname=db.name) dbExecute(conn = con, \u0026quot;CREATE TABLE IF NOT EXISTS orgs (ID INTEGER UNIQUE, City TEXT, osm_name TEXT, lon REAL,lat REAL)\u0026quot;) ## -- Iteration to web-scrap data -- ## ccount \u0026lt;- 0 ## For loop to webscrapping for(i in 1:df_len){ rg \u0026lt;- ifelse(is.null(region), '', dat[[region]][i]) st \u0026lt;- ifelse(is.null(state), '', dat[[state]][i]) ct \u0026lt;- ifelse(is.null(county), '', dat[[county]][i]) print(paste('Entry', i)) ## Do the webscrap coords \u0026lt;- coords_from_city(dat[[col.city]][i], dat[[col.country]][i], Region = rg, State = st, County = ct) ## DB send query ONLY if coords were found if(is.na(coords$lon[1])){ ccount \u0026lt;- ccount + 1 } else{ sq \u0026lt;- dbExecute(con, 'INSERT OR IGNORE INTO orgs (ID, City, osm_name, lon, lat) VALUES (?, ?, ?, ?, ?);', list(dat[['ID']][i], dat[[col.city]][i], coords$osm_name, coords$lon[1], coords$lat[1])) } print(paste('Completed', (i/df_len)*100, '%')) } ## Close db dbDisconnect(con) message(paste(\u0026quot;WEB SCRAP FOR COORDINATES SEARCH FINISHED.\u0026quot;, ccount, \u0026quot;ENTRIES NOT FOUND\u0026quot;)) } For storing the data I have chosen to use SQLite through the R package RSQLite. If you are not familiar with SQL databases I recommend you to start with a general google search and then come back to the documentation of SQLite and the R package. I chose SQLite because we needed to have something light and portable that would allow us to move the information easily from country to country rather than a centralized database where we could store everything, but a very similar approach can be applied using other types of SQL databases.\nThe function dbConnect() generates the SQLite file if it does not exist yet. Then we give SQLite the order to create the tables orgs if doesn\u0026rsquo;t exist yet, and the structure for such table. Next we search for the coordinates of the entries one by one using coords_from_city() and finally we send it to the database. In that way we could stop the process at any time and continue later by simply retrieving the table orgs from the database, compare it with our original data and move forward from what is missing. For that, the column ID is critical, it is the column that allows us to link an entry between the original data, the R data.frame and the SQL table.\nOur function also has a variable ccount that counts each time an entry was not found. In that way, once the query is finished it will print the amount of entries that were not found. The reasons for not finding an entry can be many, among the most common ones that I encountered are the following:\nWrong spelling of the City name or excess of info (i.e., value \u0026ldquo;Prague, District 3\u0026rdquo; when the city name is simply \u0026ldquo;Prague\u0026rdquo;). Wrong spelling of the State, Region and/or County name. The given City is simply not in the database of Open Street Maps (it happened specially for very small villages). Breaks of the internet connection. This one is particularly important because sometimes running the query a second or third time would find cities that were not found the first time. To read the data back to R from SQL we simply need to make a connection to the database, read the table, and close the connection. The function combine_df_sql takes care of that and at the same time joins our original data to the data stored in the database by the ID and the city name. This was important for the project because we wanted to keep the coordinates of the cities separated from the rest of the information due to some internal practical reasons. But I think that keeping all the data in SQL at once can facilitate many things. Among others, you could identify when a particular city was already found in the past and retrieve the coordinates from the database directly rather than making a connection to nominatim. I did that for a few countries and it reduces the querying time considerably. For the present post I decided to show the separated version of data in order to provide more tools to the reader.\ncombine_df_sql \u0026lt;- function(db.file, original.data){ require(dplyr) require(RSQLite) if(is.character(original.data)){ if(grepl('.csv', original.data, fixed = T)){ df \u0026lt;- read.csv(original.data) } else{ stop(\u0026quot;Incorrect file format for data\u0026quot;) } } else if(is.data.frame(original.data)){ df \u0026lt;- original.data } else{ stop(\u0026quot;Incorrect data format\u0026quot;) } con \u0026lt;- dbConnect(drv=RSQLite::SQLite(), dbname = db.file) db \u0026lt;- dbReadTable(con, \u0026quot;orgs\u0026quot;) dbDisconnect(con) result \u0026lt;- left_join(df, db, by = c('ID', 'City')) return(result) } Another detail of our function is the ability to read either from the csv file or from a data.frame. Since we were working mainly with csv files and I used data frames for the unit tests, these 2 formats were enough. Feel free to modify or extend the function for the data formats that you might need.\nMissing data As mentioned above, sometimes the results from the query would be incomplete and a second or third run were necessary but with a fewer rows. Some others I just needed to stop the query and continue later from where we left. And yet some other times the data was incomplete or wrong and this could be solved later with the data owner. The 3 scenarios required me to read the csv file to R, then the table from the database and compare them to filter the missing values. So I crafted the function compare_db_data to compare the database (db) to the original data.\ncompare_db_data \u0026lt;- function(db.file, dat){ require(dplyr) require(RSQLite) if(is.character(dat)){ if(grepl('.csv', dat, fixed = T)){ df \u0026lt;- read.csv(dat) } else{ stop(\u0026quot;Incorrect file format for data\u0026quot;) } } else if(is.data.frame(dat)){ df \u0026lt;- dat } else{ stop(\u0026quot;Incorrect data format\u0026quot;) } con \u0026lt;- dbConnect(drv=RSQLite::SQLite(), dbname = db.file) db \u0026lt;- dbReadTable(con, \u0026quot;orgs\u0026quot;) dbDisconnect(con) filtered \u0026lt;- filter(df, !(as.character(ID) %in% as.character(db$ID))) filtered } As mentioned earlier, sometimes Open Street Maps would simply not have registered certain \u0026ldquo;cities\u0026rdquo; (in fact it happened only with really small villages or populations). For that the function add_coords_manually would take a csv file with a particular structure to add the missing data. The csv file must have the following columns:\nID column named exactly like that and containing the same ID as the original data. A column containing the name of the city Columns containing the Latitude and Longitude were we want to point at the city A value for osm_name. This could be left empty or we can provide the value we want in this slot. What is important is to have the column present in the csv file. Then, as in previous function, we pass to add_coords_manually the name of the csv file with the complementary information, the name of the SQLite database and the names of the columns where we have the values for city names, osm_name, lat and long, all as strings. The rest of the function is self descriptive, provided basic knowledge of SQL syntax.\nadd_coords_manually \u0026lt;- function(csv_file, db.name, city, osm_name, lat, lon){ require(tidyverse) require(RSQLite) csv_dat \u0026lt;- read_csv(csv_file) csv_len \u0026lt;- length(csv_dat$ID) con \u0026lt;- dbConnect(drv=RSQLite::SQLite(), dbname=db.name) for(i in 1:csv_len){ dbSendQuery(con, 'INSERT OR IGNORE INTO orgs (ID, City, osm_name, lon, lat) VALUES (?, ?, ?, ?, ?);', list(csv_dat[['ID']][i], csv_dat[[city]][i], csv_dat[[osm_name]][i], csv_dat[[lat]][i], csv_dat[[lon]][i])) } dbDisconnect(con) print(paste(csv_len, 'inserted')) } Next steps If you are new to R you could probably already had noticed that one of the strengths of R that I\u0026rsquo;m using a lot here is its use of functions. The first maps that we created were done writing scripts with a few hundreds of lines. Those gave us the basis to craft the necessary functions and so, the rest of the maps were possible using just a few lines. Some of the scripts for the web scrapping of the coordinates consist of less than 10 lines of code. That is possible using the functions above and a few others created for special or particular cases. I will not share absolutely everything but I want to give an idea of how to make the process more efficient. You can always create more functions for your particular cases or modify my proposed functions to adapt to your particular situation.\nAnd speaking of extensibility, just while writting this blog I found out about the package tmaptools which contains the function geocode_OSM which uses nominatim to retrieve the coordinates of the searched point. The function has a more user friendly searching format and more possibilities for the return value, while my coords_from_city() option stays quite stiff and still with the original format that it was envisioned a few years ago when I created it. If you are truly interested in the topic I invite you to check the package. Myself I have been busy maintaining the code and creating maps that I found little time to do any improvements to the original project. But that\u0026rsquo;s exactly my main task right now so, if I do any changes to the functions presented here using the tmaptools package you can be sure that I will create a short post to share it as well.\nThen, once we got the coordinates of our target cities and we know how to make the basic map, the next step is to add the cities to the base map. In the next post I will show you how I did that and a function to make the process faster and efficient.\n","date":"2022-11-04","permalink":"https://blog.rwhitedwarf.com/post/map_any_region_with_ggplot2_part_ii/","tags":["R maps","Code Visuals","R functions","web-scrap","database"],"title":"Map any region in the world with R - Part II: Obtaining the coordinates"},{"content":"A couple of years ago I was interested in the efficiency of R when it comes to time processing and management of memory and I read a few blog posts about this topic, particularly pointing at the fact that R hasn\u0026rsquo;t been designed to be a very efficient language, especially when it comes to big data processing, and this could be its doom at some point in the future. By that time I also read a great article or blog post regarding the complexity of using the tidyverse family of packages in R, especially with the task of teaching R to beginners. The text made excellent points discussing how the syntax of tidyverse packages is so different from the base R functions that it might confuse the people trying to learn R from scratch. Thus, the narration continued towards the use of the packages data.table instead, which maintains a syntax closer to that of base R. And from there, the author also took the opportunity to discuss efficiency of both packages. I apologize for the lack of sources but I could not find the link to the post(s) I\u0026rsquo;m referring to, if any of you knows the post I\u0026rsquo;m talking about please, share the link with me, I\u0026rsquo;d be greatly thankful.\nRegardless of that line of thinking, I believe that we can all feel lucky to have packages such as tidyverse and data.table which make time processing of big data easier, among other advantages. And these are only the beginning to the list of packages. Although I was interested in the topic myself, I never run some \u0026ldquo;formal tests\u0026rdquo; to compare the efficiency of this or other packages (although I was comparing a few languages including Julia, Common Lisp and of course, Python, similarly to niklas-heer in his speed-comparison repo, to whom I also thank for my head image). Nevertheless, in the last couple of weeks I had to do such tests due to the nature of my current job.\nI recently joined a project where the team has been developing a mapper and wrapper of data using R, where the input data can vary from 2 rows to a few millions. The whole process runs through couple of servers to import the data into R, process it accordingly and send it out to a data base from where is served into some other software. The whole process per-se is quite complex and due to the use of servers and Internet connections it can become quite slow. Thus, it is critical that the time processing in R is efficient.\nAs mentioned before, a team has been working on this project for a while and they were using the tidyverse family of packages a lot. Myself I prefer to stick to base R functions when it comes to development. I think it makes the work neat, simple and easier, keeps the dependencies to the minimum and, since I know R for more than 10 years, it\u0026rsquo;s easier for me to write code in base R. And please, don\u0026rsquo;t misunderstand me, I like the tidyverse functions but I rather use them when it comes to data analysis: it is great to clean data, prepare it to fit models, explore it, and of course, to make visualizations with the wonderful ggplot preceded by the %\u0026gt;% sequence to provide exactly what is needed. But for me, developing a software in base R is just more straight forward.\nHowever, as I said, efficiency is critical in this project and thus, I\u0026rsquo;ve been tasked to test it in a few functions already. The most recent was a function to import JSON files line by line using dplyr functions which I could reduce by half the time using data.table functions, but that\u0026rsquo;s a topic for another time. One of the first tasks I was given as a new member was to map a process, very similar to another one but with different input parameters. I could had simply copied the code from the previous mapping process into my own script and just change the parameters, since the mapping logic is exactly the same. However, I decided to create my own code using base R, trusting that is more straight forward and efficient, and at the same time taking the opportunity to show up my skills to my new team. Therefore, I ended up comparing the efficiency of the functions using Monte Carlo simulations and thus, creating the present post. I hope it can be useful for some of you.\nImage 1. Credits - https://github.com/niklas-heer/speed-comparison The task The general idea is to map a RESPONSE based on the contents of one column, in this case CODE1: all values get the response \u0026ldquo;BATCH\u0026rdquo;, but only when CODE1 is empty, they also get the response \u0026ldquo;GETTING\u0026rdquo;. Rows with value \u0026ldquo;BATCH\u0026rdquo; get renamed the columns NAME, DAY and TIME into TEAM, RESPONSETD and RESPONSESTT respectively, while rows with response \u0026ldquo;GETTING\u0026rdquo; only get one more column: NAME into newly named column TEAM.\n(test.df \u0026lt;- data.frame(NAME = as.character(c(1:10)), DAY = format(Sys.time(), \u0026quot;%d-%m-%y\u0026quot;), TIME = format(Sys.time(), \u0026quot;%T\u0026quot;), CODE1 = c(\u0026quot;Code\u0026quot;, NA))) \u0026gt; NAME DAY TIME CODE1 \u0026gt; 1 1 20-10-22 18:37:23 Code \u0026gt; 2 2 20-10-22 18:37:23 \u0026lt;NA\u0026gt; \u0026gt; 3 3 20-10-22 18:37:23 Code \u0026gt; 4 4 20-10-22 18:37:23 \u0026lt;NA\u0026gt; \u0026gt; 5 5 20-10-22 18:37:23 Code \u0026gt; 6 6 20-10-22 18:37:23 \u0026lt;NA\u0026gt; \u0026gt; 7 7 20-10-22 18:37:23 Code \u0026gt; 8 8 20-10-22 18:37:23 \u0026lt;NA\u0026gt; \u0026gt; 9 9 20-10-22 18:37:23 Code \u0026gt; 10 10 20-10-22 18:37:23 \u0026lt;NA\u0026gt; The whole general idea is to create a new table with response values, which follows and is followed by a series of adjustments to the data. For the post I have created a test data frame with simple values, in case somebody would like to reproduce the code execution.\nrename_nCols \u0026lt;- function(samples, cols_to_rename, rename = FALSE, ignore_missing = TRUE){ for(i in 1:length(cols_to_rename)){ old_name \u0026lt;- cols_to_rename[[i]][1] ## Old in position 1 of vector SYS_name \u0026lt;- cols_to_rename[[i]][2] ## New in position 2 of vector ## WHEN NOT PRESENT if(!old_name %in% names(samples)) { warning(paste(\u0026quot;Column\u0026quot;, old_name, \u0026quot;not found.\u0026quot;)) if(!ignore_missing){ samples[,SYS_name] \u0026lt;- as.character(NA) } } ## RENAMING else if(rename){ names(samples)[names(samples) == old_name] \u0026lt;- SYS_name } ## ADDING else { samples[,SYS_name] \u0026lt;- samples[,old_name] } } return(samples) } create_cols_base \u0026lt;- function(samples){ require(dplyr) ## First BATCH assay \u0026lt;- cbind(samples, RESPONSE = \u0026quot;BATCH\u0026quot;) cols_to_rename \u0026lt;- list(c('NAME', 'TEAM'), c('DAY', 'RESPONSETD'), c('TIME', 'RESPONSESTT')) assay \u0026lt;- rename_nCols(assay, cols_to_rename) ## then GETTING if(\u0026quot;CODE1\u0026quot; %in% names(samples)){ if(nrow(samples[is.na(samples$CODE1),]) != 0){ receiving \u0026lt;- cbind(samples[is.na(samples$CODE1),], RESPONSE = \u0026quot;GETTING\u0026quot;) } else receiving \u0026lt;- samples[is.na(samples$CODE1),] } else{ receiving \u0026lt;- cbind(samples, RESPONSE = \u0026quot;GETTING\u0026quot;) } receiving \u0026lt;- rename_nCols(receiving, list(c('NAME', 'TEAM'))) responses \u0026lt;- full_join(assay, receiving) return(responses) } My strategy using base R (function create_cols_base()) was to create two data frames, one per each response, and then join them using full_join() from dplyr. I want to stress that the idea was never to use only base R but rather to follow my own logic and my knowledge of R and then compare it with that of my colleagues. To support my create_cols_base() I created a function rename_nCols which is a great asset to the project since we are constantly renaming columns or creating new ones based on old ones.\ncreate_cols_tidy \u0026lt;- function(samples, responsesToCreate = c(\u0026quot;BATCH\u0026quot;, \u0026quot;GETTING\u0026quot;)){ require(dplyr) require(tidyr) responses \u0026lt;- samples %\u0026gt;% mutate( RESPONSE = case_when( is.na(get0('CODE1', ifnotfound = as.character(NA))) | get0('CODE1', ifnotfound = as.character(NA)) == \u0026quot;\u0026quot; ~ list(Reduce(intersect,list(responsesToCreate,c(\u0026quot;BATCH\u0026quot;, \u0026quot;GETTING\u0026quot;)))), TRUE ~ list(Reduce(intersect,list(responsesToCreate,c(\u0026quot;BATCH\u0026quot;)))) ) ) %\u0026gt;% unnest(cols = c(RESPONSE))%\u0026gt;% subset(!is.na(RESPONSE)) %\u0026gt;% mutate(TEAM = get0('NAME', ifnotfound = as.character(NA)), RESPONSESTD = case_when( (RESPONSE == \u0026quot;BATCH\u0026quot;) ~ get0('DAY', ifnotfound = as.character(NA)), TRUE ~ as.character(NA)), RESPONSESTTM = case_when( (RESPONSE == \u0026quot;BATCH\u0026quot;) ~ get0('TIME', ifnotfound = as.character(NA)), TRUE ~ as.character(NA))) return(responses) } As you can see in the code chunk above, my colleagues decided to use a completely different approach, with the function case_when() as the protagonist. An excellent call in my opinion, but one I\u0026rsquo;m not so familiar with in R. They also made use of the strength of mutate() to reduce generation of excessive data frames, as it was my case.\nThe test Image 2. Results of the comparisons using Monte Carlo Simulations. Strong lines represent the median, long and short dotted lines the maximum and minimum values, respectively. To test the time efficiency of each function I iterated each of them a thousand times using datasets of different sizes, going from 1 thousand to 5 million, measuring the time at the beginning and end of the mapping process, and extracting the difference. The graphics presented here are the Minimum, Maximum and Median values of the thousand repetitions per each function. You can see the amount of rows in the data frame plotted against the time that each function took, in seconds.\nThe results, as expected, show a direct correlation between time and amount of rows processed. What is interesting is that up to one million rows, the increase is very slow and the difference between methods is almost not noticeable. In the image 3 we can see that differences are smaller than 1 second. However, as the amount of rows increases above a million, the differences between methods are bigger, to a point where they even double the time.\nImage 3. Results of the comparisons up to a million rows Strong lines represent the median, long and short dotted lines the maximum and minimum values, respectively. Conclusions We are not using datasets above 5 million rows in the project, and even rarely above a million so, we can afford the process to take up to 12 seconds from time to time. However, there was a nice lesson to learn, especially for me: my method using base r functions is twice as slow than a method using tidyverse group of functions. That shows the commitment of R studio of making not only more human-readable functions, but also more efficient.\nThis is also true for a series of new packages appearing in the last years that are helping R to cope better with big data. As I mentioned at the beginning of my post, I consider myself lucky to see how R is evolving and adapting to the challenges of our times when we have the needs to process big amounts of data relatively fast. Rather than see its slow time processing as its future doom, I see it as the potential where developers are focusing to create packages that can make our job easier and be up to the challenge. And for that, I thank them!.\n","date":"2022-10-20","permalink":"https://blog.rwhitedwarf.com/post/comparison_dplyr_vs_base_r/","tags":["R-dev","R tips"],"title":"Efficiency comparison of dplyr and tidyr functions vs base R"},{"content":"Scope of this post When you prepare for a job interview one of the questions they always tell you to prepare is \u0026ldquo;What are you most proud of?\u0026rdquo;. Personally I\u0026rsquo;ve never been asked that question in a job interview but it kept me thinking. Some years ago I developed the R code for the creation of maps of infrastructure for a Political Sciences project, and I can say that this is one of the projects I\u0026rsquo;m most proud of. However, it is also true what they say to developers, that nobody cares about how you did it. The final user only cared about what was done, while the research team about what are the possibilities. Due to the confidentiality agreement of the client, I also cannot share a git repository.\nThe project taught me so much in terms of technical skills that I have decided to share the how in case it can help somebody else. It is also my way to contribute to the R community since I myself learned R and programming thanks to the kind people who post their experience on the web (and to the ones who have the patience to answer questions in StackOverflow too).\nWe created maps of data showing changes over a span of time for different countries and pointing at all kinds of cities. That basically means that we need to map any region of the world with R. Today there are all kinds of packages and techniques to do that. I will share the strategy I used with ggplot2 and maps packages, using support of Open Street Map to obtain the coordinates of cities and finally making it interactive with shiny. The project is quite long for a single post, so my idea is to split it into a few smaller blog posts. The list can still change but I thought something like this:\n1. The basic map 2. Web scrapping with nominatim open street maps 3. Maps with cities 4. Dynamic maps in time 5. Making a single script for fast replication 6. Making the code interactive in a shiny app I hope you all enjoy it. Feel free to leave any kind of comment and/or question at the end.\nBackground When I joined the team all what they knew is that the wanted to make maps of infrastructure (say hospitals, cafes, churches, public offices, etc., but the project can basically be applied to anything countable per city). The maps should change in time according to the data (usually growth) and it should be possible to apply it for any country and thus, any kind of city of that particular country can be listed there. This last point represents a challenge because to make a map you need the coordinates of a particular point to map, but instead we got address in the best scenario, or only city name in the worst. Therefore, we left it to the level of city and decided to work with that.\nMost R packages to make maps have granularity up to some regions and major cities per country, and we are talking about countries where somebody has develop some R package for that. However, even the best packages would miss some cities or some countries some times. We needed to standardize everything without the need of changing packages according to the particular country. Before I joined, the team attempted to use Google Maps and excel, but the amount of data became messy and the flexibility to edit the maps was pretty limited. And they didn\u0026rsquo;t want to add copyright issues to the list of limitations. Therefore I proposed to use R. Of course, nobody in the team had ever heard about it before. We could had used any other tool, I learned that both, Python and JavaScript have some decent possibilities. But R is what I have been using for the last 10 years and is what I wanted to use for this project. And so I started to code.\nThe first couple of maps were custom code for a particular country with decent styles. But it quickly evolved into a set of functions and arguments to maintain the same standards for each map. The support of graphic designers also took the styles to a very professional level. After a few months we had very professional maps that could be done in couple of hrs (or less) with a couple of lines of code. Each map per each country with the desired span of years to be printed.\nI don\u0026rsquo;t think I will share every single detail of it, but at least I want to show how we went from the basic map to its dynamic form mapping over a span of time, and how I wrapped it all together in a couple of functions to make it quickly replicable for any given data set. Let me know what you think.\nHow to create a map of any country in R using the library maps The first step is to create the basic map of a country. Here is the function to achieve exactly that.\nlibrary(maps) library(ggplot2) ## make a df with only the country to overlap map_data_es \u0026lt;- map_data('world')[map_data('world')$region == \u0026quot;Spain\u0026quot;,] ## The map (maps + ggplot2 ) ggplot() + ## First layer: worldwide map geom_polygon(data = map_data(\u0026quot;world\u0026quot;), aes(x=long, y=lat, group = group), color = '#9c9c9c', fill = '#f3f3f3') + ## Second layer: Country map geom_polygon(data = map_data_es, aes(x=long, y=lat, group = group), color = 'red', fill = 'pink') + coord_map() + coord_fixed(1.3, xlim = c(-13.5, 8.5), ylim = c(34, 45)) + ggtitle(\u0026quot;A map of Spain\u0026quot;) + theme(panel.background =element_rect(fill = 'blue')) We are using the library maps in combination with ggplot2. The maps package contains coordinates system for a map of the whole world separated by countries (although political borders might not be fully up to date). It can as well do the maps, but for that we are making use of ggplot2 support here.\nWe start by extracting the data relevant to the country we want to map, in this case Spain. It is of course important to pass the name of the country in the same way that it is written in map_data('world')$region. You can use the function unique() to find the exact names of all the countries included in the packages (unique(map_data('world')$region) gives 252 countries at the moment of writing this post).\nOnce we have the data for the one particular country, we could simply map it directly using geom_polygon() however, that would map Spain surrounded by empty space around it. To place it in the context of its neighborhood, we apply two layers of geom_polygon(): first one with the map of the whole world and secondly the map of the country only.\nThen we need to tell ggplot to use a coordinates system to create maps instead of just polygons. For that we use coord_map() function and then we pass the details of the map ratio, and limits in X and Y to the function coord_fixed().\nUp to here we can have our map. ggplot is basically plotting what we are specifying inside the coordinates system, everything around it (the oceans) will be just empty and it will be filled in by the default grids and gray colors of ggplot(). Thus, we need to define the color of the Oceans as the background color for the whole plot. That\u0026rsquo;s what the last line of code does.\nOf course there are a lot of improvements to do. So far I have given exaggerated colors to make obvious for the reader which piece of code controls what. In that sense you can see that you can simply pass the names of the colors, which applies the defaults, or you can be more specific and provide the html notation of the color (i.e., '#9c9c9c'). So, let\u0026rsquo;s now improve the visuals and at the same time create a function to plot any country we want to.\nFunction to create the basic map in R map_country \u0026lt;- function(country, x_limits = NULL, y_limits = NULL){ ## Verifying the arguments passed to the function if(!is.character(country)) stop(\u0026quot;Name of the country should be character\u0026quot;) if(length(country) != 1) stop(\u0026quot;Function supports only one country per map\u0026quot;) ## Load libraries require(maps) require(ggplot2) if(!country %in% map_data('world')$region) stop('Country name not recognized\\nTo see a list of recognized countries run \u0026lt;unique(maps::map_data(\u0026quot;world\u0026quot;)$region)\u0026gt;') ## If coords limits missing, print worldwide map with coordinates system to allow ## User observe coords for reference if(missing(x_limits) || missing(y_limits)) { warning(\u0026quot;X and/or Y limits not provided.\\nPrinting worldwide map.\u0026quot;) map_country_theme \u0026lt;- theme(panel.background = element_rect(fill = '#4e91d2')) } else { if(length(x_limits) != 2 || length(y_limits) != 2 || !all(grepl('^-?[0-9.]+$', c(x_limits, y_limits)))){ stop(\u0026quot;Limits for X and Y coords should be provided as vectors with two numeric values\u0026quot;) } else { ## All the received inputs are correct. ## Let's define our custom theme for the final map map_country_theme \u0026lt;- theme_bw() + theme(panel.background = element_rect(fill = '#4e91d2'), legend.position = 'none', panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = \u0026quot;black\u0026quot;), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) } } ## make a df with only the country to overlap map_data_country \u0026lt;- map_data('world')[map_data('world')$region == country,] ## The map (maps + ggplot2 ) ggplot() + ## First layer: worldwide map geom_polygon(data = map_data(\u0026quot;world\u0026quot;), aes(x=long, y=lat, group = group), color = '#9c9c9c', fill = '#f3f3f3') + ## Second layer: Country map geom_polygon(data = map_data_country, aes(x=long, y=lat, group = group), color = '#4d696e', fill = '#8caeb4') + coord_map() + coord_fixed(1.3, xlim = x_limits, ylim = y_limits) + ggtitle(paste0(\u0026quot;A map of \u0026quot;, country)) + scale_x_continuous(n.breaks = 20) + scale_y_continuous(n.breaks = 20) + map_country_theme } ## Test the function with a different country map_country(\u0026quot;Germany\u0026quot;, c(-2, 22), c(47, 55)) Although the function might seem complicated at first, it is in fact the same code as we used to create the map, but instead of typing directly the name of the country or the limits for X and Y, we replace them with the arguments country, x_limits and y_limits respectively; in that way all the parts were we had the string \u0026quot;Spain\u0026quot; we now have the argument country, and so on. These are the only arguments that we need to change when we want to map a different country. You can define more arguments in case you want to have more possibilities to be editable, for example, we could define an argument country_color to specify the color we want for the target country. In our case we wanted to keep the same standards for all the maps due to branding reasons and thus, we rather wanted to have the exact same colors and styles for all of our maps.\nThere are also some additions on the top before the actual code to make the maps, all the if and else statements that are simply used to validate that the information passed by the user is the info that we actually need to make the function work. If any incorrect argument is passed to the function, we stop the process and write a message of what is wrong using the function stop(). For the case that no limits of either X or Y are defined, I send a warning message using warning(). In that case the process continues but we define a theme() that allows the user to see the country in the context of the worldwide map, with excess of values in the X and Y axes to provide the points of reference and give an idea of where to set the limits. By the end, when we ensure that all the values are fine, we define the final theme that we actually want to apply. About that, probably I should make special mention of !all(grepl('^-?[0-9.]+$', c(x_limits, y_limits)))): it is used to ensure that X and Y limits are of type numeric. See the visualization of the code below together with the help of the function(s) you don\u0026rsquo;t understand for a more detail explanation. Feel free to test the errors and warnings by providing to the function no country names or letters where there should be numbers, etc.\nThe lower part of the function is exactly the same as our first map, replacing the actual values for the arguments. We also have changed the colors for more specific ones. Almost by the end of the function we have added scale_x_continuous(n.breaks = 20) which will add 20 marks of the X axis scale (same for Y). We want to use it to ensure that, in case the user doesn\u0026rsquo;t have idea of which limit values to choose, it can have a good approach regarding the position of the target country. In case that both limits for X and Y are passed to the function, our other theme will mask this 20 breaks with axis.text.x = element_blank() and axis.ticks.x = element_blank().\nThe final line is the test that our function can plot a map other than Spain, in this case I chose Germany. We can basically choose any country included in the maps package and now make the map with the same standards in one line of R code.\nFinal remarks Here I am somehow showing one of the methods I use to create functions: I basically write first the code of what I want to achieve and once it does exactly what I want, I wrap it in a function, replacing the arguments that the user will need to modify later. Then I think what could go wrong and create the corresponding warnings an errors. It is a good practice to do that not only for the user to know better how to use the function, but also for yourself, it proves very useful when we need to debug code. Another good practice in R functions is the call to the libraries inside the function using require(). Even if you are writing many functions that use the same libraries, is good to repeat it on each function to make it self contained and again, help yourself in the debugging process. Not long ago I started collaborating in a project where there was no call to the libraries per function, but rather only at the top level when the main action of the program was called. This made almost impossible for me to test and debug code so, the first activity I did as a new member of the team was to spend 2 full working days adding require() where necessary.\nI hope you get some fun mapping different countries. Because different countries have different sizes and shapes, one way to improve the visuals related to this is by adjusting the ratio, for example, my own map of Germany looks out of shape, but it improves considerably if instead of 1.3 we give a ratio of 1.4, check the documentation to learn more about it.\nOnce that we have the basic map, we could add the cities were we want to add data values. Unfortunately, for cities there are many limitations, specially for countries where no special packages has been created to be mapped, and even there, most packages of particular countries don\u0026rsquo;t contain all the cities, especially minor ones. Thus, in our second part I will show how I tackled this problem doing some web scrapping to open street maps.\n","date":"2022-10-08","permalink":"https://blog.rwhitedwarf.com/post/map_any_region_with_ggplot2_part_i/","tags":["R maps","ggplot2","Code Visuals","R functions"],"title":"Map any region in the world with R - Part I: The basic map"},{"content":"Welcome to R minitutorials of R White Dwarf Since the beginning of this year I\u0026rsquo;ve been forced to abandon completely the blog for countless and rather abstract personal reasons that include personal health, family matters and changes in my daily activities including volunteer work as well as main job. As part of the last, I finally got hired for a position as R developer, which brings great joy to me.\nThus, I\u0026rsquo;ve been using R more lately in all kinds of forms, including review and debug of small or simple code pieces that can result in practical quick hints for other R users, especially beginners or people with not much experience using R.\nWith that aim in mind while regaining a little bit of my free time and a piece of mental stability, and celebrating my new position, I decided to take care of the blog again with simple yet useful posts called minitutorials, starting with a very simple, even silly, but useful example.\nI hope they can be useful for you or your friends. Enjoy them!\nMinitutorial: make_logical_any_string A function to make logical any string\nmake_logical_any_string \u0026lt;- function(a_string){ a_string \u0026lt;- as.character(a_string) logical_result \u0026lt;- as.logical(a_string) if(is.na(logical_result)){logical_result \u0026lt;- FALSE} return(logical_result) } The function takes any value, convert it to character and returns TRUE ONLY IF the value takes either of the following forms: \u0026quot;T\u0026quot;, \u0026quot;TRUE\u0026quot;, \u0026quot;True\u0026quot;, \u0026quot;true\u0026quot; or TRUE, the last one the logical value, not the string.\nLogic of the function The function as.character() will convert any of the true strings listed above into a logical TRUE. If the string is rather \u0026quot;False\u0026quot; or its equivalent forms, the function will return FALSE. If any other character is passed to the function, the result will be NA. Therefore, we need to tweak the results when NA\u0026rsquo;s are produced since we forcefully need a True/False result. Thus, we implement if(is.na(logical_result)){logical_result \u0026lt;- FALSE} which will force any other string to return FALSE.\nWe are using this code for running R scripts in the terminal which passes a series of arguments for its functioning, some of which are required to be TRUE only when specified so, and FALSE in any other case, hence the trick of converting any other value to FALSE rather than NA.\nSomething to keep in mind is that the arguments are always passed to R script as character and thus, I wrote the example for this post converting everything into character in the first line of the function, which is not necessary in our original code executed in the terminal. In this way, if any number is passed to the function, it will also return FALSE, emulating what would happen if a number is entered into the console. This behavior is different for the function as.logical() itself, which returns FALSE if you enter the numerical value 0 and TRUE if any other numerical value is passed.\n","date":"2022-09-18","permalink":"https://blog.rwhitedwarf.com/post/minitut_makebool/","tags":["minitutorial","R functions","R tips"],"title":"Minitutorial: make_logical any string"},{"content":"I am happy and excited as I have just deployed my first shiny app on the web. You can find it running at shiny.rwhitedwarf.com (NOTE: I don\u0026rsquo;t have ssl certificate so, your browser might tell you that is not secure, but you can trust me that there\u0026rsquo;s no risk). I have created a few shiny apps in the past but I never deployed one, especially in an owned domain.\nThe app can create a map of all cities listed in a table for a given country. The idea is to simulate a table with a list of organizations, franchises, shops, etc. However, columns for name of organization or ID are missing in order to ensure data protection. In this way the user only needs to provide to the table name of the city, country, region (optional) and year of opening. The app creates the map, placing bigger dots in cities with more organizations. The year is interactive.\nThe app uses the package RJSONIO to do a simple web scrapping on Open Street Maps using its API Nominatim to obtain the coordinates (latitude and longitude), in this way any city that can be found in open street maps can be pointed in the map. The data is then sorted and cleaned with some basic R functions and some Tidyverse and finally the map is made with ggplot2 and maps. The front end is created of course with shiny and css but I have to mention also the use of rhandsontable, a wonderful package that allows the user to interact with the tables and therefore, the data. The app was very easily deployed in heroku thanks to the wonderful work of Chris Stefano who did all the hard work of creating a buildpack for applications written in R. The buildpack recognizes shiny and plumber apps. For shiny, it builds based on the run.R file and installs all the packages listed in init.R, making the deployment in heroku extremely easy.\nThe app is still in a raw state but already functional. The plan is to improve both, the functionality and the view in the following days. If you are interested in the source code to get inspiration or create your own, you can find it on github under the MIT license: teotenn/dynamic_maps_shiny.\nIf you want to learn how to make a similar shiny app, stay connected for a 3-4 part tutorial to get full details step by step.\n","date":"2021-12-20","permalink":"https://blog.rwhitedwarf.com/post/first_shiny_app/","tags":["R shiny"],"title":"My first shiny app"},{"content":"This post is part of our series on functions in R. You can see our previous post if you want to understand the basics but it is not strictly necessary. Here we will go into detail about for loops and if statements in R, two key elements of any function. We are going to define a process, map it in a step-by-step approach and wrape it in a function that can repeat it automatically. Even if you have a very basic understanding of R you should be able to follow this tutorial without problem.\nOur outcome will be an R function that by calling it, is able to fill in empty rows generated from imported sheets (like excel) when it contains merged cells. If you are only interested in the function itself you can go to the end of the post and find it in the section Final remarks.\nDescription of the problem In our previous post we saw the basics for creating functions, yet using silly examples with not much of practical usage. Now we are ready to write a function that can have more practical use.\nThere are different ways how to import data sheets (i.e. from excel) to R. Regardless of its limitations, these sheets are widely used in data analysis today. If you are used to do data analysis with a different software you should be familiar with the complications of sorting your data imported from sheets when there are merged cells in the rows. Usually, a file like below\nresults in a table like this\nSpecie Dup Treat Rep Value A. cap A 0 1 34 AA NA NA 26 A 25 NA 18 AA NA NA 24 A 50 NA 11 AA NA NA 12 A 100 NA 15 AA NA NA 11 F. rub F 0 NA 25 FF NA NA 26 F 25 NA 17 FF NA NA 11 F 50 NA 13 FF NA NA 11 F 100 NA 11 when the amount of rows to be filled in is small, there\u0026rsquo;s no big problem in copying and pasting the values. But as the DRY principle says, if we know how to create functions there is no need to do that, we can make a function that will do it automatically. This will specially pay off when you will have a table with hundreds or even thousands of cells merged. You might be thinking that nobody will merge cells for thousands of rows every 3 or 4 lines, but believe me, I have seen such things.\nKeep in mind that this is mainly a tutorial for writting functions in R. It does not intend to deal with all the issues that migh appear with the importing of data such as merged columns or a mixture of both, among others. But if you have problems with that or are interested in the topic, leave us a comment and we can cover some points in a future post.\nR function to fill in merged cells from excel With today\u0026rsquo;s technology there are many ways to solve this problem. However sometimes the easiest way to import data to R as data analyst or statistician is by simply taking the working sheet containing the data and exporting it in csv format.\nRegardless of the source (excel, libre office, google sheets, etc.) this method produces empty rows by default. When some rows have been merged because they belong to the same group or factor, the csv file will capture the value only on the first row and leave the rest empty until the next factor appear, where it again, will capture the value on the first row and leave the rest empty until the next factor appears. The process continues like this, iteratively until the end of the table. We basically need to copy the value stored on that first row and paste it to the empty rows, until a new factor appears.\nLet\u0026rsquo;s map the process in terms of R steps to complete our task.\nMaping the process We will start by calling the table. If you have an excel, libre office calc or google sheet file with merged rows as our example above feel free to use it. Otherwise you can quickly simulate one similar to the image above. Start by exporting the sheet of interest to csv, then we call it using read.csv\nmy.table \u0026lt;- read.csv('../../../static/post/2021/fill_merged_cells/Hydroponic_results.csv') head(my.table, n = 10) \u0026gt; Specie Dup Treat Rep Value \u0026gt; 1 A. cap A 0 1 34 \u0026gt; 2 AA NA NA 26 \u0026gt; 3 A 25 NA 18 \u0026gt; 4 AA NA NA 24 \u0026gt; 5 A 50 NA 11 \u0026gt; 6 AA NA NA 12 \u0026gt; 7 A 100 NA 15 \u0026gt; 8 AA NA NA 11 \u0026gt; 9 F. rub F 0 NA 25 \u0026gt; 10 FF NA NA 26 Here we can see the first 10 rows containing NA for numeric columns and empty string for character columns. Now let\u0026rsquo;s go step by step to fill empty values.\n1. Identify and capture the factor Let\u0026rsquo;s start with the first column Specie (an experiment was run for 3 different species of plants). We want to check if the first row contains a categorical value\nno.row \u0026lt;- 1 my.table[['Specie']][no.row] \u0026gt; [1] \u0026quot;A. cap\u0026quot; 2. Copy it into the empty rows Now we want to paste the value stored in category to all empty rows. So we first need to check if the next row is empty\nno.row \u0026lt;- 2 my.table[['Specie']][no.row] \u0026gt; [1] \u0026quot;\u0026quot; And when it is, we place the value contained in the previous row to our current row 2\nmy.table[['Specie']][no.row] \u0026lt;- my.table[['Specie']][no.row-1] head(my.table) \u0026gt; Specie Dup Treat Rep Value \u0026gt; 1 A. cap A 0 1 34 \u0026gt; 2 A. cap AA NA NA 26 \u0026gt; 3 A 25 NA 18 \u0026gt; 4 AA NA NA 24 \u0026gt; 5 A 50 NA 11 \u0026gt; 6 AA NA NA 12 Now row 2 contains it\u0026rsquo;s categorical value, and when we move to row 3 (which is also empty), it can be copied from the previous row 2\nno.row \u0026lt;- 3 my.table[['Specie']][no.row] \u0026gt; [1] \u0026quot;\u0026quot; my.table[['Specie']][no.row] \u0026lt;- my.table[['Specie']][no.row-1] head(my.table) \u0026gt; Specie Dup Treat Rep Value \u0026gt; 1 A. cap A 0 1 34 \u0026gt; 2 A. cap AA NA NA 26 \u0026gt; 3 A. cap A 25 NA 18 \u0026gt; 4 AA NA NA 24 \u0026gt; 5 A 50 NA 11 \u0026gt; 6 AA NA NA 12 3. When a new factor appears, repeat the process We can repeat this process until a new factor appears, as it is the case of row 9. Therefore we should not paste anything in row 9, and continue the process on row 10 which is also empty\nno.row \u0026lt;- 10 my.table[['Specie']][no.row] \u0026gt; [1] \u0026quot;\u0026quot; my.table[['Specie']][no.row] \u0026lt;- my.table[['Specie']][no.row-1] head(my.table, n = 10) \u0026gt; Specie Dup Treat Rep Value \u0026gt; 1 A. cap A 0 1 34 \u0026gt; 2 A. cap AA NA NA 26 \u0026gt; 3 A. cap A 25 NA 18 \u0026gt; 4 AA NA NA 24 \u0026gt; 5 A 50 NA 11 \u0026gt; 6 AA NA NA 12 \u0026gt; 7 A 100 NA 15 \u0026gt; 8 AA NA NA 11 \u0026gt; 9 F. rub F 0 NA 25 \u0026gt; 10 F. rub FF NA NA 26 4. Repeat steps 1-3 for each column that needs it The process moves forward in this way until the whole column Specie is filled in. Then we can move to the next column with empty values, in my case this is Treat.\nIf you look at the process, we basically need to write an R command for step 2. The rest is just a process of verification and repetition. We are going to automate verification using the function if() and the repetition using for().\nWriting my first for loop Since the present post is directed to R beginners with not much experience with programming or coding I will avoid all the technicalities of for loops and if statements and instead dive deeply into them by applying our logic above. Then we are going to use them and explain carefully to obtain a pragmatic understanding of the process.\nWe will start with a for loop to go row by row in one column and check what is inside, as described in the step 1 of our process. Let\u0026rsquo;s start with only 20 rows as an example\nfor(no.row in 1:20){ print(my.table[[\u0026quot;Specie\u0026quot;]][no.row]) } \u0026gt; [1] \u0026quot;A. cap\u0026quot; \u0026gt; [1] \u0026quot;A. cap\u0026quot; \u0026gt; [1] \u0026quot;A. cap\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;F. rub\u0026quot; \u0026gt; [1] \u0026quot;F. rub\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;A. ela\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; \u0026gt; [1] \u0026quot;\u0026quot; The function for()uses the first argument that you define (here no.row) and goes one by one in the series specified as the second argument, in this case a series of numbers from 1 to 20 (1:20). It means that in the first iteration no.row will take the value 1, in the second iteration the value 2, and so on, until the last iteration where it will have the value 20. Each iteration will execute the code inside the curly braces {} which, in this case, is simply to print the value of each row.\nBecause the first argument in for() is defined by us, we could as well use any arbitrary argument we want, for example\nfor(n in 1:20){ print(my.table[[\u0026quot;Specie\u0026quot;]][n]) } would do exactly the same but now n is taking the values from 1 to 20.\nWe can then initialize a function that takes the name of our data frame, the name of the column, and do exactly the same as our code above, but for all the rows contained in the table, no matter how many they are\nfill_merged \u0026lt;- function(dat, column){ ## Get value of each row for(n in 1:nrow(dat)){ print(my.table[[column]][n]) } } Here our for loop will create a local variable n that will take values from 1 until nrow(dat) which means number of rows in the table dat, and then print each row value contained in the column stated in the argument column. If we apply it to our data frame\nfill_merged(my.table, \u0026quot;Specie\u0026quot;) The R console will print, one by one, each of the values contained in the column Specie (I have 120 rows, it makes no sense to take space to show it in the post, but you can go ahead and try it yourself).\nIf the last two pieces of code are not clear for you, I recommend you to read our previous post about functions in R.\nHow to use If statement in R Printing the values is far from what we want to achieve. As we defined in the first step of the process, we need to check the value inside, if it has a value we leave it alone, but if it is empty, we fill it in with the previous value. To check if the value is empty or not we use the if() function\nfill_merged \u0026lt;- function(dat, column){ ## Get value of each row for(n in 1:nrow(dat)){ ## Check if it is empty if(dat[[column]][n] == ''){ dat[[column]][n] \u0026lt;- dat[[column]][n - 1] } } return(dat) } The function if() checks the expression inside parentheses and if it returns TRUE it executes the code inside curly braces {}, otherwise it will skip it. Therefore it is crucial that the expression inside if() returns either TRUE or FALSE. You can always test it by sending the exact expression directly to the console\nmy.table[['Specie']][1] == '' \u0026gt; [1] FALSE my.table[['Specie']][5] == '' \u0026gt; [1] TRUE Once we confirm that the value is empty, we enter the if statement and execute the code from step 2, which inside the function takes the form of dat[[column]][n] \u0026lt;- dat[[column]][n - 1].If the value is not empty, we simply do nothing.\nNow we can actually try the function in one of our columns\nmy.filled.table \u0026lt;- fill_merged(my.table, 'Specie') head(my.filled.table, n = 15) \u0026gt; Specie Dup Treat Rep Value \u0026gt; 1 A. cap A 0 1 34 \u0026gt; 2 A. cap AA NA NA 26 \u0026gt; 3 A. cap A 25 NA 18 \u0026gt; 4 A. cap AA NA NA 24 \u0026gt; 5 A. cap A 50 NA 11 \u0026gt; 6 A. cap AA NA NA 12 \u0026gt; 7 A. cap A 100 NA 15 \u0026gt; 8 A. cap AA NA NA 11 \u0026gt; 9 F. rub F 0 NA 25 \u0026gt; 10 F. rub FF NA NA 26 \u0026gt; 11 F. rub F 25 NA 17 \u0026gt; 12 F. rub FF NA NA 11 \u0026gt; 13 F. rub F 50 NA 13 \u0026gt; 14 F. rub FF NA NA 11 \u0026gt; 15 F. rub F 100 NA 11 Note that so far it works only for columns with character values, not numeric\nfill_merged(my.table, 'Treat') \u0026gt; Error in if (dat[[column]][n] == \u0026quot;\u0026quot;) {: missing value where TRUE/FALSE needed The reason is that only character columns produce empty strings. Numeric columns will produce NA values. Therefore, we need to add a condition to our if expression to test also if the value is NA. To do that we use double | which in R means OR\nfill_merged \u0026lt;- function(dat, column){ ## Get value of each row for(n in 1:nrow(dat)){ ## Check if it is empty if(dat[[column]][n] == '' || is.na(dat[[column]][n])){ dat[[column]][n] \u0026lt;- dat[[column]][n - 1] } } return(dat) } Now our if statement is telling to R \u0026ldquo;IF the value is empty ('') OR is NA (is.na()) then, execute this code\u0026rdquo;, and so we enter to the code inside the if-curly-braces.\nNA are not exactly values and therefore we cannot test them by using the expression\nmy.table[['Treat']][2] == NA \u0026gt; [1] NA Instead of returning TRUE or FALSE returns NA which means Not Available. To check if the value is NA or really a value we use the function is.na()\nis.na(my.table[['Treat']][2]) \u0026gt; [1] TRUE Now we can use our function for columns with numeric values also\nmy.filled.table \u0026lt;- fill_merged(my.table, 'Treat') head(my.filled.table, n = 10) \u0026gt; Specie Dup Treat Rep Value \u0026gt; 1 A. cap A 0 1 34 \u0026gt; 2 A. cap AA 0 NA 26 \u0026gt; 3 A. cap A 25 NA 18 \u0026gt; 4 AA 25 NA 24 \u0026gt; 5 A 50 NA 11 \u0026gt; 6 AA 50 NA 12 \u0026gt; 7 A 100 NA 15 \u0026gt; 8 AA 100 NA 11 \u0026gt; 9 F. rub F 0 NA 25 \u0026gt; 10 F. rub FF 0 NA 26 Check, confirm and repeat (for and if together) Now we could use our function for each column that presents this issue, but we are actually trying to apply the DRY principle. Instead we could use for() to go through all the columns where we want to apply it. There are other functions that can help with this as well such as map() and its derived functions from the package purrr.We could also write a new function that calls our first function to repeat it into each column. Feel free to experiment, for this tutorial we are going to take a different approach.\nWe are going to improve the same old function and add yet one more for loop that iterates from each column of interest and repeats the same process. This will cover the step 4 of our mapped process.\nThe implementation is actually easy, we just need to wrap the whole previous process of if\u0026rsquo;s and for\u0026rsquo;s inside a for loop that goes column by column\nfill_merged \u0026lt;- function(dat, columns.as.vector){ ## Go through the columns for(column in columns.as.vector){ ## Get value of each row for(n in 1:nrow(dat)){ ## Check if it is empty if(dat[[column]][n] == '' || is.na(dat[[column]][n])){ dat[[column]][n] \u0026lt;- dat[[column]][n - 1] } } } return(dat) } I have changed the argument for initiating the function from column to columns.as.vector. Now this will take a vector containing all the names of the columns that should be treated. Notice that in our new for loop I am declaring column to iterate over each value of column.as.vector therefore, the variable column will contain each string in the vector for each iteration. In this way we don\u0026rsquo;t need to change the rest of the code.\nNow you can call exactly the same function for each column that needs it\ntarget_cols \u0026lt;- names(my.table)[c(1,3,4)] my.filled.table \u0026lt;- fill_merged(my.table, target_cols) head(my.filled.table, n = 15) \u0026gt; Specie Dup Treat Rep Value \u0026gt; 1 A. cap A 0 1 34 \u0026gt; 2 A. cap AA 0 1 26 \u0026gt; 3 A. cap A 25 1 18 \u0026gt; 4 A. cap AA 25 1 24 \u0026gt; 5 A. cap A 50 1 11 \u0026gt; 6 A. cap AA 50 1 12 \u0026gt; 7 A. cap A 100 1 15 \u0026gt; 8 A. cap AA 100 1 11 \u0026gt; 9 F. rub F 0 1 25 \u0026gt; 10 F. rub FF 0 1 26 \u0026gt; 11 F. rub F 25 1 17 \u0026gt; 12 F. rub FF 25 1 11 \u0026gt; 13 F. rub F 50 1 13 \u0026gt; 14 F. rub FF 50 1 11 \u0026gt; 15 F. rub F 100 1 11 Also notice the trick in the first line: names(my.table) returns a vector containing all the column names of the data frame and names(my.table)[c(1,3,4)] is taking only the columns 1, 3 and 4, which are the ones that need to be fixed. This is extremely useful when you have many columns that need to be fixed\nMarking the errors You might have noticed that I\u0026rsquo;m adding text preceded by ## within the function. If you are not familiar with it, this are comments, it means that anything that is written in the same line after one # will not be evaluated by R (I use double for technical reasons of my text editor). Although this function is quite small and simple, and we know what exactly we are doing on each line thanks to the explanations, it is a good practice to add comments to your code because after a while, when you will look back at the code you might had forgotten the logic and structure. Adding comments help us to know what each piece is about, making it easier to apply changes in the future.\nIn the same way, it is a good practice to add errors when the function is expecting something in particular and we can foresee potential problems. Often we don\u0026rsquo;t foresee all the mistakes and problems that the user or we ourselves can have when using our own functions and thus, errors are usually added along the way based on the experience gathered by using the function.\nFor example, our function is expecting that at least the first row will not have empty values, otherwise it cannot go one row before to find the value to paste on it. Although it is not expected, our table can still present this situation due to human errors, for example, somebody by accident pressed Delete button somewhere on the first row in the source file. In such case R will mark some error that will be difficult to understand and track back. We might wonder for hours what we did wrong in our function only to find out that the problem comes from the data table itself. Instead we can mark our own error in advance by sending a message when the value on the first row is missing.\nAn easy way of implementing this is using the function stop(). Let\u0026rsquo;s implement our error into our function right before it copies the value from the row n - 1.\nfill_merged \u0026lt;- function(dat, columns.as.vector){ ## Go through the columns for(column in columns.as.vector){ ## Get value of each row for(n in 1:nrow(dat)){ ## Check if it is empty if(dat[[column]][n] == '' || is.na(dat[[column]][n])){ ## If it is the row 1, stop with Error if(n == 1){ stop(paste0(\u0026quot;Row 1 of column \u0026quot;, column, \u0026quot; has empty values. Check your data.\u0026quot;)) } else{ dat[[column]][n] \u0026lt;- dat[[column]][n - 1] } } } } return(dat) } If the value of the first row is empty, the function will stop, printing a message that says in which column the row no. 1 is empty. When the row is not the first, R will evaluate the code next, contained inside else{}. else is a complement for if that tell R what to do when the if() part is not fulfilled. Sometimes we can omit it, when the if() part is not fulfilled R will simply go to the next part of the code. But when we want to make sure that nothing will happen outside these options, we include else. Here we are giving only two options, either n equals 1 and the function stops with an error, or else n is not one and the function continues.\nLet\u0026rsquo;s try the error by making a copy of our data frame with the first row empty\ntest.error \u0026lt;- my.table[2:10,] fill_merged(test.error, 'Rep') \u0026gt; Error in fill_merged(test.error, \u0026quot;Rep\u0026quot;): Row 1 of column Rep has empty values. Check your data. Another misunderstanding that the user can encounter is with the argument columns.as.vector. We are expecting that the user will provide a string, or vector of strings with the names of the columns, but the user as well might think that the function is expecting the whole data as vector. We can prevent the user for doing this by adding an error at the beginning of the function.\nfill_merged \u0026lt;- function(dat, columns.as.vector){ ## Check if column names are provided as strings if(!is.character(columns.as.vector)){ stop(\u0026quot;Column names must be provided as string or vector of strings of class character\u0026quot;) } ## Go through the columns for(column in columns.as.vector){ ## Get value of each row for(n in 1:nrow(dat)){ ## Check if it is empty if(dat[[column]][n] == '' || is.na(dat[[column]][n])){ ## If it is the row 1, stop with Error if(n == 1){ stop(paste0(\u0026quot;Row 1 of column \u0026quot;, column, \u0026quot; has empty values. Check your data.\u0026quot;)) } else{ dat[[column]][n] \u0026lt;- dat[[column]][n - 1] } } } } return(dat) } We want to make sure that columns.as.vector is character. To test this, we use the function is.character(), which will return TRUE when the values inside are character, and FALSE otherwise.\na \u0026lt;- 'foo' is.character(a) \u0026gt; [1] TRUE However we want to send the error only when the value is NOT character. To tell R to test the opposite, we start the argument with the symbol !\n!is.character(a) \u0026gt; [1] FALSE Now with our new version, when the user might provide any value that is not string, the error will be triggered\nfill_merged(my.table, 1) \u0026gt; Error in fill_merged(my.table, 1): Column names must be provided as string or vector of strings of class character Still if the user provides a vector of strings, or a misspelled name of the column, our function is not aware of it. We can add one more error when column.as.vector is string but not a string that we are expecting\nfill_merged \u0026lt;- function(dat, columns.as.vector){ ## Check if column names are provided as strings if(!is.character(columns.as.vector)){ stop(\u0026quot;Column names must be provided as string or vector of strings of class character\u0026quot;) } ## Go through the columns for(column in columns.as.vector){ ## Check if the column name matches with dat column names if (!column %in% names(dat)){ stop(paste0('Column \u0026lt;', column, '\u0026gt; cannot be found in the data frame')) } ## Get value of each row for(n in 1:nrow(dat)){ ## Check if it is empty if(dat[[column]][n] == '' || is.na(dat[[column]][n])){ ## If it is the row 1, stop with Error if(n == 1){ stop(paste0(\u0026quot;Row 1 of column \u0026lt;\u0026quot;, column, \u0026quot;\u0026gt; has empty values. Check your data.\u0026quot;)) } else{ dat[[column]][n] \u0026lt;- dat[[column]][n - 1] } } } } return(dat) } Here we are implementing if (!column %in% names(dat)). Again we are using ! to tell R to test the opposite. column %in% names(dat) will take the value of column and check if it is present in the vector names(dat) (which has the names of the columns).\nLet\u0026rsquo;s test the last error by misspelling the name of one column\nfill_merged(my.table, c('Specie', 'Treatment')) \u0026gt; Error in fill_merged(my.table, c(\u0026quot;Specie\u0026quot;, \u0026quot;Treatment\u0026quot;)): Column \u0026lt;Treatment\u0026gt; cannot be found in the data frame If you can think of more errors feel free to add them, it will be good for you as a practice. However you should also try and see what happens with other potential scenarios before adding the errors yourself. Sometimes the default errors from other functions are enough to solve problems. For example, try providing to our function a data frame that does not exist, R will immediately tell you object 'x' not found.\nFinal remarks I hope that the post has helped you to have a better understanding of for() and if(), and provided you with a good guidance on how you can plan and structure functions. If something was not clear or you still have questions, or something in your code did not work as expected, feel welcome to leave us a comment below (you will need a github account for that).\nThere are many ways how you can call your function now to your future projects. One of the easiest for now would be to save it in an R script, for example fill_merged_cells.R and then you can call it from any script or R code by providing the path to your script to the function source()\nsource('~/Rscripts/fill_merged_cells.R') changing the path to the exact location of your file. Source will run all the code contained inside the .R file in the R session where you call it, making your function available for the current session.\nHere is the final form of the function for filling in empty rows produced by merged cells. I hope it will help with your work. Enjoy it!\nfill_merged \u0026lt;- function(dat, columns.as.vector){ ## Check if column names are provided as strings if(!is.character(columns.as.vector)){ stop(\u0026quot;Column names must be provided as string or vector of strings of class character\u0026quot;) } ## Go through the columns for(column in columns.as.vector){ ## Check if the column name matches with dat column names if (!column %in% names(dat)){ stop(paste0('Column \u0026lt;', column, '\u0026gt; cannot be found in the data frame')) } ## Get value of each row for(n in 1:nrow(dat)){ ## Check if it is empty if(dat[[column]][n] == '' || is.na(dat[[column]][n])){ ## If it is the row 1, stop with Error if(n == 1){ stop(paste0(\u0026quot;Row 1 of column \u0026lt;\u0026quot;, column, \u0026quot;\u0026gt; has empty values. Check your data.\u0026quot;)) } else{ dat[[column]][n] \u0026lt;- dat[[column]][n - 1] } } } } return(dat) } ","date":"2021-12-05","permalink":"https://blog.rwhitedwarf.com/post/fill_merged_cells/","tags":["R tips","R basics","R functions"],"title":"R function to fill in merged cells"},{"content":"Background This is the first post of R with White Dwarf and I decided to start this blog with a basic tutorial. There is already a lot of information in the web about getting started with R. With a simple google search you can easily find info on how to install it, how to use R studio or other text editor, learn about the basic functions and concepts, what is a vector, a data frame, how to use them, etc. Therefore, I decided to start with a topic that is also basic and fundamental but slightly less common: Functions.\nHow to create a function is not an easy topic for non-programmers and non-mathematicians, Myself I have a background in Ecology and when I started using R for my statistical analysis I was avoiding using functions at all cost, while most of my colleagues where avoiding R fully. Many people has the idea that, as a programming language, R is really difficult to use and it should be left for the initiated ones. They end up using user interface based-software which assumes not only that the user doesn\u0026rsquo;t know about programming, but also about statistics. It makes things easy for the user but also limits the possibilities of what you can do with your data and as a result, it also what you can learn.\nIn today\u0026rsquo;s world, it is important to to have at least a basic understanding of programming. Learning how to write simple functions in R will widen your perception about R and programming by showing you that it is actually easy. I am writing this post especially for all the people who are not programmers, not statisticians and are thinking to learn R. By the end of the post I hope that you can agree with me that writing functions in R is not difficult.\nHow to write functions in R Basically, when we use R we are using functions all the time. When you want to obtain the summation of values, or the mean or standard deviation, you can simply call a function to do that\nvalues \u0026lt;- c(2, 3, 4, 5) sum(values) \u0026gt; [1] 14 As you should already know from any R tutorial, the example above is storing the values in the vector value and then calling the function sum to obtain the summation of the values. One way to create our own version of sum would be:\nmy_sum \u0026lt;- function(user.values){ cumulative.sum \u0026lt;- user.values[1] for(i in 2:length(user.values)){ cumulative.sum \u0026lt;- cumulative.sum + user.values[i] } return(cumulative.sum) } Now we can call our brand new function and obtain the same results\nmy_sum(values) \u0026gt; [1] 14 Let\u0026rsquo;s go piece by piece. Line no. 1 is simply placing the function that we are creating into the object my_sum which means that later, we can call our function using that same argument: my_func(some values). This is similar to creating a vector or data frame or variables, as you know, if you enter x \u0026lt;- 12 then each time you type x in the console it will return the value 12, and so it explains line 2, when we define cumulative.sum \u0026lt;- user.values[1] this places the first value of the vector user.values into the variable cumulative.sum. It means that now we can start by adding the second value to the cumulative.sum, then we move forward to the third value, and so on until the last element in the vector. This process is defined in the for loop: we move value by value from the second element to the last one: for(i in 2:length(user.values)), each time we stored the cumulative value in our variable cumulative.sum until we reach the last value. I will not go deep into the for loop, but I understand that it can also be somehow complicated for a beginner, if it is your case I invite you to leave us a comment (you will need a github account for that) and I might cover it in a future issue.\nOnce we are outside the for loop we have collected the final value in cumulative.sum so, we make sure that our function is returning exactly that by using return(cumulative.sum). If you have seen some other tutorials you might have noticed that the return() is not always added at the end of the function. And indeed, it is not strictly necessary (more on that later), but as a beginner it is good to start with good habits and defining what exactly you want your function to return is a good habit for your future functions.\nSimple error handling When you work with functions you need to tell the user what exactly went wrong in order to help him fix it. Even if you are writing functions only for yourself, after a while has passed you might forget all the logic behind your function and thus, obtaining errors that you don\u0026rsquo;t understand where they come from. A basic knowledge of error handling can help us prevent that.\nWhat I\u0026rsquo;m explaining here is a very basic and simple management of errors but yet, practical and useful, it can save us wasted time and headaches. It is something I wish I had learned when I started writing my first functions. Due to my ignorance it used to take me a lot of time just to figure out what was wrong with my own code.\nLet\u0026rsquo;s go back to our function. As you probably already noticed, it starts summing up from the second value in the vector, therefore if we provide only one value instead of a vector of values the result will be NA\nmy_sum(12) \u0026gt; [1] NA quite silly compared to the professional function from base-R which returns the value itself\nsum(12) \u0026gt; [1] 12 We could try to imitate the base-R sum() and continue in that direction, but instead we are going to have a little fun with simple examples of errors. Let\u0026rsquo;s say that instead of returning the value itself, we want our new function to send an error when a single value is entered. For that, we simply need to check if the value size is bigger than 1, and if not, send the error. We can achieve that with an if statement:\nmy_sum \u0026lt;- function(user.values){ if(length(user.values) == 1){ stop('We cannot sum individual values here!') } cumulative.sum \u0026lt;- user.values[1] for(i in 2:length(user.values)){ cumulative.sum \u0026lt;- cumulative.sum + user.values[i] } return(cumulative.sum) } As you can see in line 2, we will enter inside the if-part-of-code if the length of the values is one (we cannot have length smaller than 1, if we run the function without a value, R will say that the argument is missing), calling stop() which basically stops the function at that point, and exits printing whatever message you define inside it. Go ahead and try it.\nmy_sum(12) \u0026gt; Error in my_sum(12): We cannot sum individual values here! I am sure that with this basic info you can already move forward and improve it even more to send an error message when an object other than a vector is entered. Try to do it yourself and feel free to leave me a comment below if you get any trouble. Some hints: You can use the function is.vector() to test if the value entered by the user is a vector or not; and you can place one if statement inside the other, first to check if it is a vector, and secondly to check its size.\nFunction arguments You might be wondering what about the argument used as variable user.values, where does it come from? how is it defined? how does R knows how to use it? Keeping it simple, all the arguments that you define inside the parenthesis of a function will be searched by R when you execute the function and will be used accordingly. You can easily see how we were using the variable user.values to tell the rest of the program what to do with it. The function has no idea if the user will enter a single value, a vector or a data frame, this is the reason why we created the errors with stop(). As the creator of the function, it is your role to decide what kind of object you need, how to use it and how to ensure that the user knows what is wrong if an unexpected object is entered.\nYou can define as many arguments as you wish for your function, for example\nsum_four_nums \u0026lt;- function(num1, num2, num3, num4){ return(sum(num1, num2, num3, num4)) } sum_four_nums(2, 4, 6, 8) \u0026gt; [1] 20 Here we are telling R to take the four values entered by the user and sum them up. R will check the values in the order they are entered, so in our example it will associate the value 2 with our first variable num1, then the value 4 with the second variable num2 and so on. If we miss one of the values, R will tell us that one of the variables is missing\nsum_four_nums(2, 4, 6) \u0026gt; Error in sum_four_nums(2, 4, 6): argument \u0026quot;num4\u0026quot; is missing, with no default If we want to allow the user to provide only 3 values, we can initialize one of them as null\nsum_four_nums \u0026lt;- function(num1, num2, num3, num4 = NULL){ return(sum(num1, num2, num3, num4)) } sum_four_nums(2, 4, 6) \u0026gt; [1] 12 This means that we can actually initialize our variables with whatever we want to put on it, for example we can tell our function to always add 10 if only 3 values are entered by the user\nsum_four_nums \u0026lt;- function(num1, num2, num3, num4 = 10){ return(sum(num1, num2, num3, num4)) } sum_four_nums(2, 4, 6) \u0026gt; [1] 22 Also notice that we are telling R to take strictly four values, and not a vector of size 4. If we do this, R will associate the vector to the variable num1 as one object and will complain that the other arguments are missing\nsum_four_nums(c(2, 4, 6, 8)) \u0026gt; Error in sum_four_nums(c(2, 4, 6, 8)): argument \u0026quot;num2\u0026quot; is missing, with no default As I mentioned already, R is not aware of what type of object the user should enter, therefore we could as well enter only a vector, or vector and numbers, and R will simply apply the sum() function to whatever is inside it, because this is how we defined our function\nsum_four_nums(c(2, 4, 6, 8), 20, 50) \u0026gt; [1] 100 Here R is summing first all values contained in the vector, then 20 and 50, and finally the predefined 10. As you can see, the proper handling of errors is important when you want to ensure that you function does what is intended to do, or to help you or the user identify what exactly when wrong.\nFunctions without arguments You can also define functions without arguments, meaning without direct input from the user. For example, let\u0026rsquo;s write the classical Hello World!, a function that, when called, prints the sentence itself\nhello_world_function \u0026lt;- function(){ print('Hello World!') } hello_world_function() \u0026gt; [1] \u0026quot;Hello World!\u0026quot; As you can see, in line 1 when we define the function there is nothing inside the parenthesis and thus, when we call the function we don\u0026rsquo;t need to include anything inside it. This example might look silly, but sometimes we want the functions for their side effects, rather than for the values they return.\nWhen we write a function, R will search for the variable inside the function\nsum_my_vector \u0026lt;- function(){ my.vector \u0026lt;- c(10, 20, 30) return(sum(my.vector)) } sum_my_vector() \u0026gt; [1] 60 ls() \u0026gt; [1] \u0026quot;base.dir\u0026quot; \u0026quot;base.url\u0026quot; \u0026quot;changing.wd\u0026quot; \u0026gt; [4] \u0026quot;dirs\u0026quot; \u0026quot;fig.path\u0026quot; \u0026quot;func.params\u0026quot; \u0026gt; [7] \u0026quot;hello_world_function\u0026quot; \u0026quot;my_sum\u0026quot; \u0026quot;rmd.file\u0026quot; \u0026gt; [10] \u0026quot;rmd.path\u0026quot; \u0026quot;sum_four_nums\u0026quot; \u0026quot;sum_my_vector\u0026quot; \u0026gt; [13] \u0026quot;values\u0026quot; \u0026quot;work.in\u0026quot; As you can see, the vector called my.vector is created inside the function and thus, when we call it, the function returns the sum of the vector. However, when we list all the objects in memory using ls(), the object my.vector doesn\u0026rsquo;t exists. All the objects that we define inside the function live only there. If we now create an object called my.vector and call again the function, the result will not change\nmy.vector \u0026lt;- c(1, 2, 3) sum_my_vector() \u0026gt; [1] 60 ls() \u0026gt; [1] \u0026quot;base.dir\u0026quot; \u0026quot;base.url\u0026quot; \u0026quot;changing.wd\u0026quot; \u0026gt; [4] \u0026quot;dirs\u0026quot; \u0026quot;fig.path\u0026quot; \u0026quot;func.params\u0026quot; \u0026gt; [7] \u0026quot;hello_world_function\u0026quot; \u0026quot;my_sum\u0026quot; \u0026quot;my.vector\u0026quot; \u0026gt; [10] \u0026quot;rmd.file\u0026quot; \u0026quot;rmd.path\u0026quot; \u0026quot;sum_four_nums\u0026quot; \u0026gt; [13] \u0026quot;sum_my_vector\u0026quot; \u0026quot;values\u0026quot; \u0026quot;work.in\u0026quot; The reason is that R functions search for objects inside the function. Therefore, you could give the same names to objects inside and outside the functions without affecting the outcome, however this is not recommended because it might cause confusion in the future. Another reason why is not recommended is that R searches for the object inside the function first, but when it cannot find it, it searches for the object outside of the function, in your working environment (it means, what we can see listed by ls()), for example\nsum_other_vector \u0026lt;- function(){ return(sum(my.vector)) } sum_other_vector() \u0026gt; [1] 6 here I have created a similar function but this time I did not create the object my.vector inside it, therefore R is using the one that I loaded into the working environment as my.vector \u0026lt;- c(1, 2, 3).\nWe could consider the objects created inside the function as local variables because they have local effect only, and the ones defined outside of the function as global variables. Other programming languages make a clear difference of this two and handle each of them differently, often by initiating the global variables with special characters, or creating them using special functions, in order to avoid mistakes and confusion. In R you should be very careful on how you name your objects and where you use them when you are creating functions.\nOn the other hand it has the advantage that it is very easy to create functions that use the same structure of data. For example, I could create a data frame called elements that will always contain the columns called Pb, As, Cd and Zn and then just make functions that take no arguments to do all my statistics at once by calling the same table and the same columns inside them.\nWhy to write functions As mentioned above, I started writing functions when I did my Ph.D. I was working with contaminated soils and basically for all my projects I had to analyze data of concentration of elements. This means that for each project, I had to repeat the same process for each element and then, for the next project do the same for the new data and for different or more elements. Luckily my first project was only focused on 4 different elements. I did a script for the statistics and visualizations of the first element, organize the workflow, decided what would be variable and what constant, and created two functions, one for the statistics and one for the visualizations, based on the output of the first one, and then just applied the functions to the remaining 3 elements.\nWhen I got the first results of my second experiment it was related to more than 10 different elements, and that only for soils, I knew that later I\u0026rsquo;d have to do the same statistics for different parts of the plants. Therefore I decided to create a package. I simply googled how to put all my functions together in a package, installed it and then, for each of my next data results I could simply call my own functions directly in my R environment from any folder and do all the statistical analysis way faster than I can even measure.\nLearning how to write functions in R is not only intended for processes or calculations that don\u0026rsquo;t have a particular function yet (today basically everything is cover in one or another package). It can save a lot of time in any kind of work you are doing. It can reduce the time you need for your data analysis and the amount of code written in your scripts. As a result it also makes your code more organized and more understandable. It can also help you to understand better how R works, as you need to get more familiar with the type of objects used, the structures of the functions, the application of conditionals and iterative processes, etc.\nFunctions are a key element of most (probably all) programming languages and thus, learning how to create your own will also develop your programming skills and teach you how to automatize tasks. There is a general informal rule for programming that is called the DRY principle, which means Don\u0026rsquo;t Repeat Yourself. In other words, if there is a process in your code/program/script that has to be repeated at least once, it is worth it to write a function and then call it twice with the different arguments that will be variable rather than coping the whole code from the first case and pasting it where the second case needs it and only changing the arguments that are variable in the second case. The next post will be exactly about that.\nFinal remarks I hope that this tutorial has reach its goal of showing how easy and useful is to write your own functions in R. I agree that all the functions created here had minimum practical application. It is usually the case when getting started. But right in our next post we are going to write our first complete function with practical application: A function that fills empty rows generated from merged cells imported from excel.\nStay in touch!\n","date":"2021-11-30","permalink":"https://blog.rwhitedwarf.com/post/functions/","tags":["R tips","R basics","R functions"],"title":"Functions in R"}]